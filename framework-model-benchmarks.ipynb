{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPXW/jrWeBQEOHgK2cQNU4i"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "628b206134554ec9b7417799f51ae4e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_639edd667aaa434eb22644e081c43254",
              "IPY_MODEL_5a046b231ba147b1b2394509f9f2d225",
              "IPY_MODEL_ee65e57463874ecea733fcce7cb4caa2"
            ],
            "layout": "IPY_MODEL_9dd367fde4dd47e882fd821e7650fccb"
          }
        },
        "639edd667aaa434eb22644e081c43254": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e9492e1583304212843189bea7fa1cd0",
            "placeholder": "​",
            "style": "IPY_MODEL_e2d6c0e610f94eef813bd5e6e5940220",
            "value": "Phi-3.5-mini-instruct-Q4_K_M.gguf: 100%"
          }
        },
        "5a046b231ba147b1b2394509f9f2d225": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9465cb17d869406498793f2c7bc7c81e",
            "max": 2393232672,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6de765ae525b4d5d94749c0f21bdbdfe",
            "value": 2393232672
          }
        },
        "ee65e57463874ecea733fcce7cb4caa2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4b25b65a58a4515a9be94ba757323d2",
            "placeholder": "​",
            "style": "IPY_MODEL_1dddcf376b794b9283e09bcbdff2a695",
            "value": " 2.39G/2.39G [01:06&lt;00:00, 151MB/s]"
          }
        },
        "9dd367fde4dd47e882fd821e7650fccb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9492e1583304212843189bea7fa1cd0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2d6c0e610f94eef813bd5e6e5940220": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9465cb17d869406498793f2c7bc7c81e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6de765ae525b4d5d94749c0f21bdbdfe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c4b25b65a58a4515a9be94ba757323d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1dddcf376b794b9283e09bcbdff2a695": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4c6ccdb9c9c049ba8fd334b27d2500ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a73279bd4c6846159ab09edf91ded01f",
              "IPY_MODEL_982b34a314e54802a04204a966e071cd",
              "IPY_MODEL_0295a9d7445e48acb8b701ac0a072bf8"
            ],
            "layout": "IPY_MODEL_929821f81f094875899172cd4e4c21ea"
          }
        },
        "a73279bd4c6846159ab09edf91ded01f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1d9a0089c05419ca09ac28a2aa59f3f",
            "placeholder": "​",
            "style": "IPY_MODEL_de94aaf6bd0445f0be5dd4a1b85c196e",
            "value": "README.md: "
          }
        },
        "982b34a314e54802a04204a966e071cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca125503149e408a8f88fceec3ae32c0",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bf7e3aca7dfa4564a5d4b4c79141a826",
            "value": 1
          }
        },
        "0295a9d7445e48acb8b701ac0a072bf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8421fc2453114c8387d84e8a555bf061",
            "placeholder": "​",
            "style": "IPY_MODEL_755b21339d2840b8bdd7da96fc3d674c",
            "value": " 7.94k/? [00:00&lt;00:00, 756kB/s]"
          }
        },
        "929821f81f094875899172cd4e4c21ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1d9a0089c05419ca09ac28a2aa59f3f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de94aaf6bd0445f0be5dd4a1b85c196e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ca125503149e408a8f88fceec3ae32c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "bf7e3aca7dfa4564a5d4b4c79141a826": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8421fc2453114c8387d84e8a555bf061": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "755b21339d2840b8bdd7da96fc3d674c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5ebdf76863424894a1f31c2cec96e82e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5f05b1a10c9540a3b9708a4c4be261ce",
              "IPY_MODEL_70a790aac0804b77b18b8d34d1cacf42",
              "IPY_MODEL_c3d95350e82d43ed9c003ddcdeb7aca9"
            ],
            "layout": "IPY_MODEL_c94e12582bf248528f11860bb2cbcd4d"
          }
        },
        "5f05b1a10c9540a3b9708a4c4be261ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7eb65c291fc6496385106b2558c8c09f",
            "placeholder": "​",
            "style": "IPY_MODEL_44dcfe306c0a4b78a42a08c80fa37c65",
            "value": "main/train-00000-of-00001.parquet: 100%"
          }
        },
        "70a790aac0804b77b18b8d34d1cacf42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_88cc9814b8ef4c559b92477aaf40efec",
            "max": 2306545,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6145e85274cd44bbb8365468cec0f00c",
            "value": 2306545
          }
        },
        "c3d95350e82d43ed9c003ddcdeb7aca9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ec8e87dad23403ab2f8cee36ce175bf",
            "placeholder": "​",
            "style": "IPY_MODEL_9af274ac79ee436d952c320c075da4b9",
            "value": " 2.31M/2.31M [00:00&lt;00:00, 15.6kB/s]"
          }
        },
        "c94e12582bf248528f11860bb2cbcd4d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7eb65c291fc6496385106b2558c8c09f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44dcfe306c0a4b78a42a08c80fa37c65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "88cc9814b8ef4c559b92477aaf40efec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6145e85274cd44bbb8365468cec0f00c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1ec8e87dad23403ab2f8cee36ce175bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9af274ac79ee436d952c320c075da4b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "94f03e45d37f4882b5c666edd1432117": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4717a001b87545d7a7e9760cc3017903",
              "IPY_MODEL_566cf3f72d224711a4e7d81eaf85b9b5",
              "IPY_MODEL_557f83d7f3424d97a2a4798f3ea181aa"
            ],
            "layout": "IPY_MODEL_355c25d7cca24528b1a83c827a65d565"
          }
        },
        "4717a001b87545d7a7e9760cc3017903": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f462cffa48c44df8c572ec6f2a584b7",
            "placeholder": "​",
            "style": "IPY_MODEL_0b06e1a1cbad410f8596b358cbe63b36",
            "value": "main/test-00000-of-00001.parquet: 100%"
          }
        },
        "566cf3f72d224711a4e7d81eaf85b9b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7dff9f18549645a7ae470d2806766657",
            "max": 419088,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_96e8ad432b004966904a71741b354b6b",
            "value": 419088
          }
        },
        "557f83d7f3424d97a2a4798f3ea181aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54c478e6ecb74c759c9e3e47aa8d01fc",
            "placeholder": "​",
            "style": "IPY_MODEL_900bbfc2483e406192bbcdd18af77b80",
            "value": " 419k/419k [00:00&lt;00:00, 2.03MB/s]"
          }
        },
        "355c25d7cca24528b1a83c827a65d565": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f462cffa48c44df8c572ec6f2a584b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b06e1a1cbad410f8596b358cbe63b36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7dff9f18549645a7ae470d2806766657": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96e8ad432b004966904a71741b354b6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "54c478e6ecb74c759c9e3e47aa8d01fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "900bbfc2483e406192bbcdd18af77b80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "76c4e489e78e4b93ae6a59af33848b88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1e2c930ab39a4932b628701b4737a093",
              "IPY_MODEL_dd942ae335774b35a9b5ea7f3596858a",
              "IPY_MODEL_eb7f2fe18b964e5a834e57cb8e86e8e5"
            ],
            "layout": "IPY_MODEL_127ac5f755d4415db632518e30460bd2"
          }
        },
        "1e2c930ab39a4932b628701b4737a093": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3f3fa9cb14a469e94c5d65ec8dc0c4f",
            "placeholder": "​",
            "style": "IPY_MODEL_d5afb999a98146cbb50e8199a7092086",
            "value": "Generating train split: 100%"
          }
        },
        "dd942ae335774b35a9b5ea7f3596858a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_904a285029204a82924908bf26f4a720",
            "max": 7473,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_798485c7ac3b46c5bccb55220d42e598",
            "value": 7473
          }
        },
        "eb7f2fe18b964e5a834e57cb8e86e8e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_05b18aa949be4b21baf0e7f7cd6b70a0",
            "placeholder": "​",
            "style": "IPY_MODEL_668499dcd4dc4beca64b6746bb3b9830",
            "value": " 7473/7473 [00:00&lt;00:00, 137392.48 examples/s]"
          }
        },
        "127ac5f755d4415db632518e30460bd2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3f3fa9cb14a469e94c5d65ec8dc0c4f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5afb999a98146cbb50e8199a7092086": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "904a285029204a82924908bf26f4a720": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "798485c7ac3b46c5bccb55220d42e598": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "05b18aa949be4b21baf0e7f7cd6b70a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "668499dcd4dc4beca64b6746bb3b9830": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "07c5b775beb0455ebc0da157732682a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e0af170fd2b34dfd8ef4ee613a2ef9a5",
              "IPY_MODEL_65f3ea306e0646fa83ec4f400bea6306",
              "IPY_MODEL_a90dbb42410e44d39872aca41f828a8c"
            ],
            "layout": "IPY_MODEL_c93c67a0be954b4a99e6e88b1e31df82"
          }
        },
        "e0af170fd2b34dfd8ef4ee613a2ef9a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a26b3af653d46748bbf2b5abea9fdf2",
            "placeholder": "​",
            "style": "IPY_MODEL_41a853be546245e5b2ca7d24bf86fe53",
            "value": "Generating test split: 100%"
          }
        },
        "65f3ea306e0646fa83ec4f400bea6306": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b580b72fba346e1a422006f0fe1df56",
            "max": 1319,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_990098d0455644e38694b3f505c53573",
            "value": 1319
          }
        },
        "a90dbb42410e44d39872aca41f828a8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d3d8a9dfab964d17a06adb2e2af1e3ae",
            "placeholder": "​",
            "style": "IPY_MODEL_7a811d4e7ed1425fb9fad81fb86ab8b4",
            "value": " 1319/1319 [00:00&lt;00:00, 35917.18 examples/s]"
          }
        },
        "c93c67a0be954b4a99e6e88b1e31df82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a26b3af653d46748bbf2b5abea9fdf2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41a853be546245e5b2ca7d24bf86fe53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0b580b72fba346e1a422006f0fe1df56": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "990098d0455644e38694b3f505c53573": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d3d8a9dfab964d17a06adb2e2af1e3ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a811d4e7ed1425fb9fad81fb86ab8b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Environment (GPU T4)"
      ],
      "metadata": {
        "id": "5Vkuc_Dq1aFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi\n",
        "!pip -q uninstall -y llama-cpp-python llama-cpp-python-cu121 llama-cpp-python-cu122 llama-cpp-python-cu124 || true\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "hpxIPmpv0mSI",
        "outputId": "63a96fa2-77a1-4e39-b683-63cbbc9bd415"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Oct  5 03:52:23 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   55C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "\u001b[33mWARNING: Skipping llama-cpp-python as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping llama-cpp-python-cu121 as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping llama-cpp-python-cu122 as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping llama-cpp-python-cu124 as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sanity\n",
        "!python -V\n",
        "!pip -V\n",
        "\n",
        "# Clean any old bits (safe if not installed)\n",
        "!pip -q uninstall -y llama-cpp-python || true\n",
        "\n",
        "# Install the CUDA 12.4 (cu124) prebuilt wheel\n",
        "!pip install --upgrade --force-reinstall --no-cache-dir \\\n",
        "  llama-cpp-python \\\n",
        "  --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu124\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1035
        },
        "id": "5QRjz81YXGYC",
        "outputId": "92851079-07a7-4130-b88e-8dd85b138092"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.12.11\n",
            "pip 24.1.2 from /usr/local/lib/python3.12/dist-packages/pip (python 3.12)\n",
            "\u001b[33mWARNING: Skipping llama-cpp-python as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://abetlen.github.io/llama-cpp-python/whl/cu124\n",
            "Collecting llama-cpp-python\n",
            "  Downloading https://github.com/abetlen/llama-cpp-python/releases/download/v0.3.16-cu124/llama_cpp_python-0.3.16-cp312-cp312-linux_x86_64.whl (551.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m551.3/551.3 MB\u001b[0m \u001b[31m109.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions>=4.5.0 (from llama-cpp-python)\n",
            "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting numpy>=1.20.0 (from llama-cpp-python)\n",
            "  Downloading numpy-2.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting jinja2>=2.11.3 (from llama-cpp-python)\n",
            "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2>=2.11.3->llama-cpp-python)\n",
            "  Downloading markupsafe-3.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.7 kB)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m174.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.9/134.9 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m230.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m159.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading markupsafe-3.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (22 kB)\n",
            "Installing collected packages: typing-extensions, numpy, MarkupSafe, diskcache, jinja2, llama-cpp-python\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.15.0\n",
            "    Uninstalling typing_extensions-4.15.0:\n",
            "      Successfully uninstalled typing_extensions-4.15.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 3.0.3\n",
            "    Uninstalling MarkupSafe-3.0.3:\n",
            "      Successfully uninstalled MarkupSafe-3.0.3\n",
            "  Attempting uninstall: jinja2\n",
            "    Found existing installation: Jinja2 3.1.6\n",
            "    Uninstalling Jinja2-3.1.6:\n",
            "      Successfully uninstalled Jinja2-3.1.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.3 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.3 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.3 which is incompatible.\n",
            "cupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.3.3 which is incompatible.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.3.3 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed MarkupSafe-3.0.3 diskcache-5.6.3 jinja2-3.1.6 llama-cpp-python-0.3.16 numpy-2.3.3 typing-extensions-4.15.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "70f050c7ff114da9828fb659ab75b95a"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download model"
      ],
      "metadata": {
        "id": "RMF669gx3Dly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install -U \"huggingface_hub>=0.24.0\"\n",
        "\n",
        "from huggingface_hub import list_repo_files, hf_hub_download, login\n",
        "import os, pathlib\n",
        "\n",
        "def download_gguf_model(\n",
        "    repo_id: str,\n",
        "    quant_preference: str = \"Q4_K_M\",\n",
        "    filename: str | None = None,\n",
        "    dest_dir: str = \"/content/models\",\n",
        "    hf_token: str | None = None,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Download a GGUF model from Hugging Face.\n",
        "    - If `filename` is set, fetch that file.\n",
        "    - Else pick the first *.gguf containing `quant_preference` (case-insensitive),\n",
        "      falling back to the first *.gguf if none match.\n",
        "    Returns the local path.\n",
        "    \"\"\"\n",
        "    # Only login if a token is provided (avoid the Colab secrets warning)\n",
        "    if hf_token:\n",
        "        login(token=hf_token)\n",
        "\n",
        "    dest = pathlib.Path(dest_dir)\n",
        "    dest.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    files = list_repo_files(repo_id, token=hf_token)\n",
        "    ggufs = [f for f in files if f.lower().endswith(\".gguf\")]\n",
        "    if not ggufs:\n",
        "        raise FileNotFoundError(f\"No .gguf files found in {repo_id}\")\n",
        "\n",
        "    if filename:\n",
        "        if filename not in files:\n",
        "            raise FileNotFoundError(f\"{filename} not found in {repo_id}\")\n",
        "        target = filename\n",
        "    else:\n",
        "        cand = [f for f in ggufs if quant_preference.lower() in f.lower()]\n",
        "        target = sorted(cand)[0] if cand else sorted(ggufs)[0]\n",
        "\n",
        "    # Be compatible with different huggingface_hub versions\n",
        "    try:\n",
        "        path = hf_hub_download(\n",
        "            repo_id=repo_id,\n",
        "            filename=target,\n",
        "            local_dir=dest,\n",
        "            token=hf_token,\n",
        "            local_dir_use_symlinks=False,  # preferred kwarg (plural)\n",
        "        )\n",
        "    except TypeError:\n",
        "        # Older versions don’t support local_dir_use_symlinks; retry without it\n",
        "        path = hf_hub_download(\n",
        "            repo_id=repo_id,\n",
        "            filename=target,\n",
        "            local_dir=dest,\n",
        "            token=hf_token,\n",
        "        )\n",
        "\n",
        "    print(f\"Downloaded: {path}\")\n",
        "    return path\n",
        "\n",
        "# Example: Phi-3.5 Mini Instruct GGUF (public community conversions)\n",
        "PHI35_GGUF_REPO = \"bartowski/Phi-3.5-mini-instruct-GGUF\"  # fallback below if needed\n",
        "\n",
        "try:\n",
        "    MODEL_PATH = download_gguf_model(\n",
        "        repo_id=PHI35_GGUF_REPO,\n",
        "        quant_preference=\"Q4_K_M\",\n",
        "        dest_dir=\"/content/models\"\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(\"Primary repo failed, trying TheBloke mirror…\", e)\n",
        "    MODEL_PATH = download_gguf_model(\n",
        "        repo_id=\"TheBloke/phi-3.5-mini-instruct-GGUF\",\n",
        "        quant_preference=\"Q4_K_M\",\n",
        "        dest_dir=\"/content/models\"\n",
        "    )\n",
        "\n",
        "print(\"MODEL_PATH =\", MODEL_PATH)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260,
          "referenced_widgets": [
            "628b206134554ec9b7417799f51ae4e9",
            "639edd667aaa434eb22644e081c43254",
            "5a046b231ba147b1b2394509f9f2d225",
            "ee65e57463874ecea733fcce7cb4caa2",
            "9dd367fde4dd47e882fd821e7650fccb",
            "e9492e1583304212843189bea7fa1cd0",
            "e2d6c0e610f94eef813bd5e6e5940220",
            "9465cb17d869406498793f2c7bc7c81e",
            "6de765ae525b4d5d94749c0f21bdbdfe",
            "c4b25b65a58a4515a9be94ba757323d2",
            "1dddcf376b794b9283e09bcbdff2a695"
          ]
        },
        "id": "5bUYlxup3DXC",
        "outputId": "b187a20a-d944-4fe3-de08-e42d3a02247b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:982: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
            "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Phi-3.5-mini-instruct-Q4_K_M.gguf:   0%|          | 0.00/2.39G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "628b206134554ec9b7417799f51ae4e9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded: /content/models/Phi-3.5-mini-instruct-Q4_K_M.gguf\n",
            "MODEL_PATH = /content/models/Phi-3.5-mini-instruct-Q4_K_M.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create model and prompts skeleton"
      ],
      "metadata": {
        "id": "n2nTxIkc1dX3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time, re, json, math, uuid, pathlib, statistics as stats\n",
        "from dataclasses import dataclass, asdict\n",
        "import numpy as np, pandas as pd\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
        "from llama_cpp import Llama\n",
        "\n",
        "llm = Llama(\n",
        "    model_path=MODEL_PATH,\n",
        "    n_ctx=4096,\n",
        "    n_gpu_layers=-1,   # offload as many layers as fit on the T4\n",
        "    n_batch=512,       # good throughput on T4 for a 7B Q4 model\n",
        "    logits_all=True,\n",
        "    seed=1234,\n",
        "    verbose=True       # banner should say \"CUDA = 1\" and \"offloading ... layers to GPU\"\n",
        ")\n",
        "\n",
        "def run_llm(prompt, max_tokens=16, temperature=0.0, stop=None):\n",
        "    t0 = time.time()\n",
        "    out = llm(\n",
        "        prompt,\n",
        "        max_tokens=max_tokens,\n",
        "        temperature=temperature,\n",
        "        top_p=1.0,\n",
        "        stop=stop or [],\n",
        "        echo=False,\n",
        "        logprobs=5  # enables optional confidence metrics\n",
        "    )\n",
        "    dt = time.time() - t0\n",
        "    txt = out[\"choices\"][0][\"text\"]\n",
        "    usage = out.get(\"usage\", {})\n",
        "    comp_toks = usage.get(\"completion_tokens\", None)\n",
        "    toks_per_s = (comp_toks/dt) if (comp_toks and dt>0) else None\n",
        "    # first-token logprobs (for calibration/confidence)\n",
        "    first_lp = None\n",
        "    try:\n",
        "        first_lp = out[\"choices\"][0][\"logprobs\"][\"content\"][0][\"top_logprobs\"]\n",
        "    except Exception:\n",
        "        pass\n",
        "    return txt, dt, toks_per_s, first_lp\n",
        "\n",
        "# ---------- Prompt templates ----------\n",
        "def mcq_prompt(q, A, B, C, D):\n",
        "    return (\n",
        "    \"You are a careful reasoner. Choose the single best answer and reply with ONLY the letter.\\n\"\n",
        "    f\"Question: {q}\\n\\nOptions:\\nA. {A}\\nB. {B}\\nC. {C}\\nD. {D}\\n\\nFinal answer (A/B/C/D) only:\"\n",
        "    )\n",
        "\n",
        "def gsm_prompt(q):\n",
        "    return (\n",
        "    \"Solve step by step. At the end, reply with ONLY the final number.\\n\"\n",
        "    f\"Problem: {q}\\n\\nFinal answer (number only):\"\n",
        "    )\n",
        "\n",
        "def parse_letter(s):\n",
        "    m = re.search(r'\\b([ABCD])\\b', s.strip())\n",
        "    return m.group(1) if m else None\n",
        "\n",
        "def parse_number(s):\n",
        "    # last number in the string\n",
        "    m = re.findall(r'-?\\d+(?:\\.\\d+)?', s.replace(',', ''))\n",
        "    return m[-1] if m else None\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "qVgosZr00npF",
        "outputId": "de2c3d21-a222-40bd-d668-7612a7d6b1b4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    yes\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "llama_model_load_from_file_impl: using device CUDA0 (Tesla T4) - 14992 MiB free\n",
            "llama_model_loader: loaded meta data with 40 key-value pairs and 197 tensors from /content/models/Phi-3.5-mini-instruct-Q4_K_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = phi3\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Phi 3.5 Mini Instruct\n",
            "llama_model_loader: - kv   3:                           general.finetune str              = instruct\n",
            "llama_model_loader: - kv   4:                           general.basename str              = Phi-3.5\n",
            "llama_model_loader: - kv   5:                         general.size_label str              = mini\n",
            "llama_model_loader: - kv   6:                            general.license str              = mit\n",
            "llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/microsoft/Phi-...\n",
            "llama_model_loader: - kv   8:                               general.tags arr[str,3]       = [\"nlp\", \"code\", \"text-generation\"]\n",
            "llama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"multilingual\"]\n",
            "llama_model_loader: - kv  10:                        phi3.context_length u32              = 131072\n",
            "llama_model_loader: - kv  11:  phi3.rope.scaling.original_context_length u32              = 4096\n",
            "llama_model_loader: - kv  12:                      phi3.embedding_length u32              = 3072\n",
            "llama_model_loader: - kv  13:                   phi3.feed_forward_length u32              = 8192\n",
            "llama_model_loader: - kv  14:                           phi3.block_count u32              = 32\n",
            "llama_model_loader: - kv  15:                  phi3.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv  16:               phi3.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv  17:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  18:                  phi3.rope.dimension_count u32              = 96\n",
            "llama_model_loader: - kv  19:                        phi3.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  20:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  21:              phi3.attention.sliding_window u32              = 262144\n",
            "llama_model_loader: - kv  22:              phi3.rope.scaling.attn_factor f32              = 1.190238\n",
            "llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = default\n",
            "llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,32064]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  26:                      tokenizer.ggml.scores arr[f32,32064]   = [-1000.000000, -1000.000000, -1000.00...\n",
            "llama_model_loader: - kv  27:                  tokenizer.ggml.token_type arr[i32,32064]   = [3, 3, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  28:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  29:                tokenizer.ggml.eos_token_id u32              = 32000\n",
            "llama_model_loader: - kv  30:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  31:            tokenizer.ggml.padding_token_id u32              = 32000\n",
            "llama_model_loader: - kv  32:               tokenizer.ggml.add_bos_token bool             = false\n",
            "llama_model_loader: - kv  33:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  34:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...\n",
            "llama_model_loader: - kv  35:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  36:                      quantize.imatrix.file str              = /models_out/Phi-3.5-mini-instruct-GGU...\n",
            "llama_model_loader: - kv  37:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\n",
            "llama_model_loader: - kv  38:             quantize.imatrix.entries_count i32              = 128\n",
            "llama_model_loader: - kv  39:              quantize.imatrix.chunks_count i32              = 151\n",
            "llama_model_loader: - type  f32:   67 tensors\n",
            "llama_model_loader: - type q4_K:   81 tensors\n",
            "llama_model_loader: - type q5_K:   32 tensors\n",
            "llama_model_loader: - type q6_K:   17 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q4_K - Medium\n",
            "print_info: file size   = 2.23 GiB (5.01 BPW) \n",
            "load_hparams: Phi SWA is currently disabled - results might be suboptimal for some models (see https://github.com/ggml-org/llama.cpp/pull/13676)\n",
            "init_tokenizer: initializing tokenizer for type 1\n",
            "load: control token:  32008 '<|placeholder5|>' is not marked as EOG\n",
            "load: control token:  32006 '<|system|>' is not marked as EOG\n",
            "load: control token:  32002 '<|placeholder1|>' is not marked as EOG\n",
            "load: control token:  32001 '<|assistant|>' is not marked as EOG\n",
            "load: control token:  32004 '<|placeholder3|>' is not marked as EOG\n",
            "load: control token:  32003 '<|placeholder2|>' is not marked as EOG\n",
            "load: control token:      0 '<unk>' is not marked as EOG\n",
            "load: control token:  32005 '<|placeholder4|>' is not marked as EOG\n",
            "load: control token:  32010 '<|user|>' is not marked as EOG\n",
            "load: control token:  32009 '<|placeholder6|>' is not marked as EOG\n",
            "load: control token:      1 '<s>' is not marked as EOG\n",
            "load: printing all EOG tokens:\n",
            "load:   - 32000 ('<|endoftext|>')\n",
            "load:   - 32007 ('<|end|>')\n",
            "load: special tokens cache size = 14\n",
            "load: token to piece cache size = 0.1685 MB\n",
            "print_info: arch             = phi3\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 131072\n",
            "print_info: n_embd           = 3072\n",
            "print_info: n_layer          = 32\n",
            "print_info: n_head           = 32\n",
            "print_info: n_head_kv        = 32\n",
            "print_info: n_rot            = 96\n",
            "print_info: n_swa            = 0\n",
            "print_info: is_swa_any       = 0\n",
            "print_info: n_embd_head_k    = 96\n",
            "print_info: n_embd_head_v    = 96\n",
            "print_info: n_gqa            = 1\n",
            "print_info: n_embd_k_gqa     = 3072\n",
            "print_info: n_embd_v_gqa     = 3072\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-05\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 8192\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 2\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 10000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 4096\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: model type       = 3B\n",
            "print_info: model params     = 3.82 B\n",
            "print_info: general.name     = Phi 3.5 Mini Instruct\n",
            "print_info: vocab type       = SPM\n",
            "print_info: n_vocab          = 32064\n",
            "print_info: n_merges         = 0\n",
            "print_info: BOS token        = 1 '<s>'\n",
            "print_info: EOS token        = 32000 '<|endoftext|>'\n",
            "print_info: EOT token        = 32007 '<|end|>'\n",
            "print_info: UNK token        = 0 '<unk>'\n",
            "print_info: PAD token        = 32000 '<|endoftext|>'\n",
            "print_info: LF token         = 13 '<0x0A>'\n",
            "print_info: EOG token        = 32000 '<|endoftext|>'\n",
            "print_info: EOG token        = 32007 '<|end|>'\n",
            "print_info: max token length = 48\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: layer   0 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   1 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   2 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   3 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   4 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   5 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   6 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   7 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   8 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   9 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  10 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  11 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  12 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  13 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  14 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  15 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  16 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  17 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  18 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  19 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  20 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  21 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  22 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  23 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  24 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  25 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  26 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  27 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  28 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  29 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  30 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  31 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  32 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: tensor 'token_embd.weight' (q4_K) (and 0 others) cannot be used with preferred buffer type CUDA_Host, using CPU instead\n",
            "load_tensors: offloading 32 repeating layers to GPU\n",
            "load_tensors: offloading output layer to GPU\n",
            "load_tensors: offloaded 33/33 layers to GPU\n",
            "load_tensors:        CUDA0 model buffer size =  2228.82 MiB\n",
            "load_tensors:   CPU_Mapped model buffer size =    52.84 MiB\n",
            "............................................................................................\n",
            "llama_context: constructing llama_context\n",
            "llama_context: n_seq_max     = 1\n",
            "llama_context: n_ctx         = 4096\n",
            "llama_context: n_ctx_per_seq = 4096\n",
            "llama_context: n_batch       = 512\n",
            "llama_context: n_ubatch      = 512\n",
            "llama_context: causal_attn   = 1\n",
            "llama_context: flash_attn    = 0\n",
            "llama_context: kv_unified    = false\n",
            "llama_context: freq_base     = 10000.0\n",
            "llama_context: freq_scale    = 1\n",
            "llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "set_abort_callback: call\n",
            "llama_context:  CUDA_Host  output buffer size =     0.12 MiB\n",
            "create_memory: n_ctx = 4096 (padded)\n",
            "llama_kv_cache_unified: layer   0: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   1: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   2: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   3: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   4: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   5: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   6: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   7: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   8: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   9: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  10: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  11: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  12: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  13: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  14: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  15: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  16: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  17: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  18: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  19: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  20: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  21: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  22: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  23: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  24: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  25: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  26: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  27: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  28: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  29: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  30: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  31: dev = CUDA0\n",
            "llama_kv_cache_unified:      CUDA0 KV buffer size =  1536.00 MiB\n",
            "llama_kv_cache_unified: size = 1536.00 MiB (  4096 cells,  32 layers,  1/1 seqs), K (f16):  768.00 MiB, V (f16):  768.00 MiB\n",
            "llama_context: enumerating backends\n",
            "llama_context: backend_ptrs.size() = 2\n",
            "llama_context: max_nodes = 1576\n",
            "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
            "llama_context:      CUDA0 compute buffer size =   288.01 MiB\n",
            "llama_context:  CUDA_Host compute buffer size =    26.01 MiB\n",
            "llama_context: graph nodes  = 1126\n",
            "llama_context: graph splits = 2\n",
            "CUDA : ARCHS = 500,520,530,600,610,620,700,720,750,800,860,870,890,900 | FORCE_MMQ = 1 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
            "Model metadata: {'quantize.imatrix.chunks_count': '151', 'quantize.imatrix.entries_count': '128', 'quantize.imatrix.dataset': '/training_dir/calibration_datav3.txt', 'quantize.imatrix.file': '/models_out/Phi-3.5-mini-instruct-GGUF/Phi-3.5-mini-instruct.imatrix', 'general.quantization_version': '2', 'tokenizer.chat_template': \"{% for message in messages %}{% if message['role'] == 'system' and message['content'] %}{{'<|system|>\\n' + message['content'] + '<|end|>\\n'}}{% elif message['role'] == 'user' %}{{'<|user|>\\n' + message['content'] + '<|end|>\\n'}}{% elif message['role'] == 'assistant' %}{{'<|assistant|>\\n' + message['content'] + '<|end|>\\n'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\\n' }}{% else %}{{ eos_token }}{% endif %}\", 'phi3.rope.scaling.original_context_length': '4096', 'general.architecture': 'phi3', 'phi3.rope.scaling.attn_factor': '1.190238', 'general.license': 'mit', 'phi3.context_length': '131072', 'general.type': 'model', 'general.license.link': 'https://huggingface.co/microsoft/Phi-3.5-mini-instruct/resolve/main/LICENSE', 'tokenizer.ggml.pre': 'default', 'general.basename': 'Phi-3.5', 'tokenizer.ggml.padding_token_id': '32000', 'phi3.attention.head_count': '32', 'phi3.attention.head_count_kv': '32', 'phi3.attention.layer_norm_rms_epsilon': '0.000010', 'phi3.embedding_length': '3072', 'phi3.rope.dimension_count': '96', 'general.finetune': 'instruct', 'general.file_type': '15', 'phi3.rope.freq_base': '10000.000000', 'phi3.attention.sliding_window': '262144', 'phi3.block_count': '32', 'tokenizer.ggml.model': 'llama', 'phi3.feed_forward_length': '8192', 'general.name': 'Phi 3.5 Mini Instruct', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '32000', 'general.size_label': 'mini', 'tokenizer.ggml.add_bos_token': 'false', 'tokenizer.ggml.add_eos_token': 'false'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Using gguf chat template: {% for message in messages %}{% if message['role'] == 'system' and message['content'] %}{{'<|system|>\n",
            "' + message['content'] + '<|end|>\n",
            "'}}{% elif message['role'] == 'user' %}{{'<|user|>\n",
            "' + message['content'] + '<|end|>\n",
            "'}}{% elif message['role'] == 'assistant' %}{{'<|assistant|>\n",
            "' + message['content'] + '<|end|>\n",
            "'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\n",
            "' }}{% else %}{{ eos_token }}{% endif %}\n",
            "Using chat eos_token: <|endoftext|>\n",
            "Using chat bos_token: <s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sanity check"
      ],
      "metadata": {
        "id": "dyENcxOFdW4X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "out = llm(\"Say hello.\", max_tokens=8, temperature=0)\n",
        "print(out[\"choices\"][0][\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "oFYiCciFdVYP",
        "outputId": "0e8208bb-d1b1-45c2-ab9e-44338bb3bf6a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      58.30 ms /     3 tokens (   19.43 ms per token,    51.45 tokens per second)\n",
            "llama_perf_context_print:        eval time =     141.81 ms /     7 runs   (   20.26 ms per token,    49.36 tokens per second)\n",
            "llama_perf_context_print:       total time =     204.99 ms /    10 tokens\n",
            "llama_perf_context_print:    graphs reused =          6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Chatbot: Hello!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "t0 = time.time()\n",
        "out = llm(\"Write a haiku about code.\", max_tokens=32, temperature=0)\n",
        "dt = time.time() - t0\n",
        "toks = out.get(\"usage\", {}).get(\"completion_tokens\", 0)\n",
        "print(f\"Latency: {dt:.2f}s  |  toks/sec: {toks/dt if dt>0 else None:.1f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "pcWYYORcdoBK",
        "outputId": "b72a85e5-b04b-4f31-880b-18867d8916bc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =     328.41 ms /     7 tokens (   46.92 ms per token,    21.31 tokens per second)\n",
            "llama_perf_context_print:        eval time =     529.82 ms /    31 runs   (   17.09 ms per token,    58.51 tokens per second)\n",
            "llama_perf_context_print:       total time =     872.92 ms /    38 tokens\n",
            "llama_perf_context_print:    graphs reused =         29\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Latency: 0.88s  |  toks/sec: 36.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Datasets loaders (MMLU + GSM8K)"
      ],
      "metadata": {
        "id": "iurxoniB1pre"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MMLU (validation for DEV, test for FINAL)\n",
        "from datasets import load_dataset\n",
        "from itertools import chain\n",
        "\n",
        "# Full subject list in case we need the per-config fallback\n",
        "MMLU_SUBJECTS = [\n",
        " 'abstract_algebra','anatomy','astronomy','business_ethics','clinical_knowledge',\n",
        " 'college_biology','college_chemistry','college_computer_science','college_mathematics',\n",
        " 'college_medicine','college_physics','computer_security','conceptual_physics','econometrics',\n",
        " 'electrical_engineering','elementary_mathematics','formal_logic','global_facts',\n",
        " 'high_school_biology','high_school_chemistry','high_school_computer_science',\n",
        " 'high_school_european_history','high_school_geography','high_school_government_and_politics',\n",
        " 'high_school_macroeconomics','high_school_mathematics','high_school_microeconomics',\n",
        " 'high_school_physics','high_school_psychology','high_school_statistics','high_school_us_history',\n",
        " 'high_school_world_history','human_aging','human_sexuality','international_law','jurisprudence',\n",
        " 'logical_fallacies','machine_learning','management','marketing','medical_genetics','miscellaneous',\n",
        " 'moral_disputes','moral_scenarios','nutrition','philosophy','prehistory','professional_accounting',\n",
        " 'professional_law','professional_medicine','professional_psychology','public_relations',\n",
        " 'security_studies','sociology','us_foreign_policy','virology','world_religions'\n",
        "]\n",
        "\n",
        "def mmlu_iter(split=\"validation\", limit=None, prefer_all=True):\n",
        "    \"\"\"\n",
        "    Yields dicts with fields:\n",
        "      id, q, A, B, C, D, gold (A/B/C/D), subject\n",
        "    \"\"\"\n",
        "    count = 0\n",
        "    try:\n",
        "        if prefer_all:\n",
        "            ds = load_dataset(\"cais/mmlu\", \"all\", split=split)\n",
        "            for i, ex in enumerate(ds):\n",
        "                if limit and count >= limit: break\n",
        "                A, B, C, D = ex[\"choices\"]\n",
        "                yield {\n",
        "                    \"id\": f\"mmlu-{i}\",\n",
        "                    \"q\": ex[\"question\"],\n",
        "                    \"A\": A, \"B\": B, \"C\": C, \"D\": D,\n",
        "                    \"gold\": ex[\"answer\"],\n",
        "                    \"subject\": ex[\"subject\"],\n",
        "                }\n",
        "                count += 1\n",
        "            return\n",
        "    except Exception as e:\n",
        "        print(\"Falling back to per-subject loading:\", e)\n",
        "\n",
        "    # Fallback: iterate each subject config and chain\n",
        "    for subj in MMLU_SUBJECTS:\n",
        "        ds = load_dataset(\"cais/mmlu\", subj, split=split)\n",
        "        for j, ex in enumerate(ds):\n",
        "            if limit and count >= limit: return\n",
        "            A, B, C, D = ex[\"choices\"]\n",
        "            yield {\n",
        "                \"id\": f\"mmlu-{subj}-{j}\",\n",
        "                \"q\": ex[\"question\"],\n",
        "                \"A\": A, \"B\": B, \"C\": C, \"D\": D,\n",
        "                \"gold\": ex[\"answer\"],\n",
        "                \"subject\": subj,\n",
        "            }\n",
        "            count += 1\n",
        "\n",
        "\n",
        "# GSM8K (test for FINAL)\n",
        "def gsm8k_iter(split=\"test\", limit=None):\n",
        "    ds = load_dataset(\"gsm8k\", \"main\", split=split)\n",
        "    for i, ex in enumerate(ds):\n",
        "        if limit and i >= limit: break\n",
        "        q = ex[\"question\"]\n",
        "        # gold after '####'\n",
        "        m = re.search(r\"####\\s*(-?\\d+(?:\\.\\d+)?)\", ex[\"answer\"].replace(',', ''))\n",
        "        gold = m.group(1) if m else None\n",
        "        yield {\"id\": f\"gsm-{i}\", \"q\": q, \"gold\": gold}\n"
      ],
      "metadata": {
        "id": "0aULmmzU0z-7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Runners + metrics"
      ],
      "metadata": {
        "id": "-ykYrMpz1u84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- runners + metrics (drop-in cell) ----------------------------------------\n",
        "# Requires that you already defined elsewhere in the notebook:\n",
        "#   - mmlu_iter(split, limit), gsm8k_iter(split, limit)\n",
        "#   - run_llm(prompt, max_tokens, temperature, stop=None) -> (text, latency_s, toks_per_s, top_logprobs)\n",
        "#   - mcq_prompt(q, A, B, C, D), gsm_prompt(q)\n",
        "#   - parse_letter(text) -> raw letter-ish prediction; parse_number(text) -> numeric/str for GSM8K\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "from dataclasses import dataclass\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
        "\n",
        "ABCD = [\"A\", \"B\", \"C\", \"D\"]\n",
        "\n",
        "@dataclass\n",
        "class RunCfg:\n",
        "    model_name: str\n",
        "    quant: str\n",
        "    temperature: float = 0.0\n",
        "    seed: int = 1234\n",
        "\n",
        "\n",
        "def norm_choice(x):\n",
        "    \"\"\"Normalize any 'choice' signal to a single letter 'A'..'D'. Return None if not parseable.\"\"\"\n",
        "    if x is None:\n",
        "        return None\n",
        "\n",
        "    # Handle NaN (from pandas) quietly\n",
        "    try:\n",
        "        if isinstance(x, float) and np.isnan(x):\n",
        "            return None\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    s = str(x).strip().upper()\n",
        "    if not s:\n",
        "        return None\n",
        "\n",
        "    # direct single letter\n",
        "    if s in ABCD:\n",
        "        return s\n",
        "\n",
        "    # patterns like 'A.', 'B)', '(C)', 'Answer: D', 'option c'\n",
        "    m = re.search(r'([ABCD])(?=[\\)\\].,:;\\s]|$)', s)\n",
        "    if m:\n",
        "        return m.group(1)\n",
        "\n",
        "    # numbers 1..4 or 0..3\n",
        "    m = re.search(r'\\b([1-4])\\b', s)\n",
        "    if m:\n",
        "        return ABCD[int(m.group(1)) - 1]\n",
        "    m = re.search(r'\\b([0-3])\\b', s)\n",
        "    if m:\n",
        "        return ABCD[int(m.group(1))]\n",
        "\n",
        "    # last resort: first char if A-D\n",
        "    if s and s[0] in ABCD:\n",
        "        return s[0]\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "def evaluate_mmlu(cfg: RunCfg, split: str = \"validation\", limit=None):\n",
        "    rows = []\n",
        "    for ex in mmlu_iter(split, limit):\n",
        "        prompt = mcq_prompt(ex[\"q\"], ex[\"A\"], ex[\"B\"], ex[\"C\"], ex[\"D\"])\n",
        "        out, dt, tps, top = run_llm(\n",
        "            prompt,\n",
        "            max_tokens=8,\n",
        "            temperature=cfg.temperature,\n",
        "            stop=[\"\\n\"],\n",
        "        )\n",
        "        pred = parse_letter(out)  # keep your parser; we'll normalize below\n",
        "        rows.append({\n",
        "            \"id\": ex[\"id\"],\n",
        "            \"subject\": ex[\"subject\"],\n",
        "            \"gold\": ex[\"gold\"],\n",
        "            \"pred\": pred,\n",
        "            \"latency_s\": dt,\n",
        "            \"toks_per_s\": tps,\n",
        "            \"raw\": out,\n",
        "            \"top_logprobs\": top,\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "\n",
        "    # Normalize AFTER collection to avoid mixing types\n",
        "    df[\"gold_norm\"] = df[\"gold\"].apply(norm_choice)\n",
        "    df[\"pred_norm\"] = df[\"pred\"].apply(norm_choice)\n",
        "\n",
        "    # Mark correctness on normalized labels (invalid rows become False)\n",
        "    df[\"correct\"] = (\n",
        "        df[\"pred_norm\"].isin(ABCD)\n",
        "        & df[\"gold_norm\"].isin(ABCD)\n",
        "        & (df[\"pred_norm\"] == df[\"gold_norm\"])\n",
        "    )\n",
        "\n",
        "    # Evaluate only rows where both sides are valid letters\n",
        "    eval_mask = df[\"gold_norm\"].isin(ABCD) & df[\"pred_norm\"].isin(ABCD)\n",
        "    eval_df = df[eval_mask].copy()\n",
        "\n",
        "    labels = ABCD\n",
        "    if len(eval_df):\n",
        "        y_true = eval_df[\"gold_norm\"].tolist()\n",
        "        y_pred = eval_df[\"pred_norm\"].tolist()\n",
        "        acc = float(eval_df[\"correct\"].mean())\n",
        "        prec, rec, f1, _ = precision_recall_fscore_support(\n",
        "            y_true, y_pred, labels=labels, average=\"macro\", zero_division=0\n",
        "        )\n",
        "        cm = confusion_matrix(y_true, y_pred, labels=labels).tolist()\n",
        "    else:\n",
        "        acc, prec, rec, f1 = 0.0, 0.0, 0.0, 0.0\n",
        "        cm = [[0] * 4 for _ in range(4)]\n",
        "\n",
        "    # per-subject accuracy (only on evaluated rows so coverage gaps don’t penalize)\n",
        "    by_subj = (\n",
        "        eval_df.groupby(\"subject\")[\"correct\"].mean().sort_values(ascending=False)\n",
        "        if len(eval_df) else pd.Series(dtype=float)\n",
        "    )\n",
        "    by_subj = {k: float(v) for k, v in by_subj.to_dict().items()}\n",
        "\n",
        "    # latency stats across all processed rows (independent of label validity)\n",
        "    lat = {\n",
        "        \"p50\": float(df[\"latency_s\"].median()) if len(df) else 0.0,\n",
        "        \"p90\": float(df[\"latency_s\"].quantile(0.9)) if len(df) else 0.0,\n",
        "        \"mean\": float(df[\"latency_s\"].mean()) if len(df) else 0.0,\n",
        "    }\n",
        "    # throughput\n",
        "    tps_series = df[\"toks_per_s\"].dropna()\n",
        "    perf = {\"tps_mean\": float(tps_series.mean()) if not tps_series.empty else None}\n",
        "\n",
        "    metrics = {\n",
        "        \"n_total\": int(len(df)),\n",
        "        \"n_eval\": int(len(eval_df)),\n",
        "        \"coverage\": float(len(eval_df) / len(df) if len(df) else 0.0),\n",
        "        \"accuracy\": float(acc),\n",
        "        \"precision_macro\": float(prec),\n",
        "        \"recall_macro\": float(rec),\n",
        "        \"f1_macro\": float(f1),\n",
        "        \"confusion_matrix\": cm,\n",
        "        \"latency\": lat,\n",
        "        \"throughput\": perf,\n",
        "        \"per_subject_accuracy\": by_subj,\n",
        "    }\n",
        "\n",
        "    return df, metrics\n",
        "\n",
        "\n",
        "def evaluate_gsm8k(cfg: RunCfg, split: str = \"test\", limit=None):\n",
        "    rows = []\n",
        "    for ex in gsm8k_iter(split, limit):\n",
        "        out, dt, tps, top = run_llm(\n",
        "            gsm_prompt(ex[\"q\"]),\n",
        "            max_tokens=128,\n",
        "            temperature=cfg.temperature,\n",
        "        )\n",
        "        pred = parse_number(out)\n",
        "        rows.append({\n",
        "            \"id\": ex[\"id\"],\n",
        "            \"gold\": ex[\"gold\"],\n",
        "            \"pred\": pred,\n",
        "            \"latency_s\": dt,\n",
        "            \"toks_per_s\": tps,\n",
        "            \"raw\": out,\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "\n",
        "    # strict EM accuracy\n",
        "    df[\"correct\"] = (df[\"pred\"].astype(str) == df[\"gold\"].astype(str))\n",
        "    acc = float(df[\"correct\"].mean())\n",
        "\n",
        "    # format-error rate (non-numeric outputs)\n",
        "    fmt_err = float(df[\"pred\"].isna().mean())\n",
        "\n",
        "    # latency + throughput (across all rows)\n",
        "    lat = {\n",
        "        \"p50\": float(df[\"latency_s\"].median()) if len(df) else 0.0,\n",
        "        \"p90\": float(df[\"latency_s\"].quantile(0.9)) if len(df) else 0.0,\n",
        "        \"mean\": float(df[\"latency_s\"].mean()) if len(df) else 0.0,\n",
        "    }\n",
        "    tps = df[\"toks_per_s\"].dropna()\n",
        "    perf = {\"tps_mean\": float(tps.mean()) if not tps.empty else None}\n",
        "\n",
        "    metrics = {\n",
        "        \"n_total\": int(len(df)),\n",
        "        \"accuracy_em\": acc,\n",
        "        \"format_error_rate\": fmt_err,\n",
        "        \"latency\": lat,\n",
        "        \"throughput\": perf,\n",
        "    }\n",
        "\n",
        "    return df, metrics\n",
        "# ---------------------------------------------------------------------------\n"
      ],
      "metadata": {
        "id": "cnEwW3WV02zn"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run a smoke test (fast)"
      ],
      "metadata": {
        "id": "-M4dIrFB1zll"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = RunCfg(model_name=\"phi-3.5-mini-instruct\", quant=\"Q4_K_M\")\n",
        "\n",
        "mmlu_df, mmlu_metrics = evaluate_mmlu(cfg, split=\"validation\", limit=200)   # quick\n",
        "gsm_df,  gsm_metrics  = evaluate_gsm8k(cfg, split=\"test\", limit=100)        # quick\n",
        "\n",
        "print(\"MMLU:\", json.dumps(mmlu_metrics, indent=2)[:800], \"…\")\n",
        "print(\"GSM8K:\", json.dumps(gsm_metrics, indent=2)[:800], \"…\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "4c6ccdb9c9c049ba8fd334b27d2500ab",
            "a73279bd4c6846159ab09edf91ded01f",
            "982b34a314e54802a04204a966e071cd",
            "0295a9d7445e48acb8b701ac0a072bf8",
            "929821f81f094875899172cd4e4c21ea",
            "e1d9a0089c05419ca09ac28a2aa59f3f",
            "de94aaf6bd0445f0be5dd4a1b85c196e",
            "ca125503149e408a8f88fceec3ae32c0",
            "bf7e3aca7dfa4564a5d4b4c79141a826",
            "8421fc2453114c8387d84e8a555bf061",
            "755b21339d2840b8bdd7da96fc3d674c",
            "5ebdf76863424894a1f31c2cec96e82e",
            "5f05b1a10c9540a3b9708a4c4be261ce",
            "70a790aac0804b77b18b8d34d1cacf42",
            "c3d95350e82d43ed9c003ddcdeb7aca9",
            "c94e12582bf248528f11860bb2cbcd4d",
            "7eb65c291fc6496385106b2558c8c09f",
            "44dcfe306c0a4b78a42a08c80fa37c65",
            "88cc9814b8ef4c559b92477aaf40efec",
            "6145e85274cd44bbb8365468cec0f00c",
            "1ec8e87dad23403ab2f8cee36ce175bf",
            "9af274ac79ee436d952c320c075da4b9",
            "94f03e45d37f4882b5c666edd1432117",
            "4717a001b87545d7a7e9760cc3017903",
            "566cf3f72d224711a4e7d81eaf85b9b5",
            "557f83d7f3424d97a2a4798f3ea181aa",
            "355c25d7cca24528b1a83c827a65d565",
            "9f462cffa48c44df8c572ec6f2a584b7",
            "0b06e1a1cbad410f8596b358cbe63b36",
            "7dff9f18549645a7ae470d2806766657",
            "96e8ad432b004966904a71741b354b6b",
            "54c478e6ecb74c759c9e3e47aa8d01fc",
            "900bbfc2483e406192bbcdd18af77b80",
            "76c4e489e78e4b93ae6a59af33848b88",
            "1e2c930ab39a4932b628701b4737a093",
            "dd942ae335774b35a9b5ea7f3596858a",
            "eb7f2fe18b964e5a834e57cb8e86e8e5",
            "127ac5f755d4415db632518e30460bd2",
            "a3f3fa9cb14a469e94c5d65ec8dc0c4f",
            "d5afb999a98146cbb50e8199a7092086",
            "904a285029204a82924908bf26f4a720",
            "798485c7ac3b46c5bccb55220d42e598",
            "05b18aa949be4b21baf0e7f7cd6b70a0",
            "668499dcd4dc4beca64b6746bb3b9830",
            "07c5b775beb0455ebc0da157732682a5",
            "e0af170fd2b34dfd8ef4ee613a2ef9a5",
            "65f3ea306e0646fa83ec4f400bea6306",
            "a90dbb42410e44d39872aca41f828a8c",
            "c93c67a0be954b4a99e6e88b1e31df82",
            "6a26b3af653d46748bbf2b5abea9fdf2",
            "41a853be546245e5b2ca7d24bf86fe53",
            "0b580b72fba346e1a422006f0fe1df56",
            "990098d0455644e38694b3f505c53573",
            "d3d8a9dfab964d17a06adb2e2af1e3ae",
            "7a811d4e7ed1425fb9fad81fb86ab8b4"
          ]
        },
        "id": "Pv8faVcH1zV2",
        "outputId": "c7a5aac9-5ac5-4eba-a9da-575e4088ff31",
        "collapsed": true
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 24 prefix-match hit, remaining 56 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      75.65 ms /    56 tokens (    1.35 ms per token,   740.27 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      78.73 ms /    57 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 54 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      68.40 ms /    54 tokens (    1.27 ms per token,   789.49 tokens per second)\n",
            "llama_perf_context_print:        eval time =      18.07 ms /     1 runs   (   18.07 ms per token,    55.33 tokens per second)\n",
            "llama_perf_context_print:       total time =      89.67 ms /    55 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 95 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      75.20 ms /    95 tokens (    0.79 ms per token,  1263.38 tokens per second)\n",
            "llama_perf_context_print:        eval time =      15.86 ms /     1 runs   (   15.86 ms per token,    63.05 tokens per second)\n",
            "llama_perf_context_print:       total time =      95.51 ms /    96 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 65 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      50.33 ms /    65 tokens (    0.77 ms per token,  1291.40 tokens per second)\n",
            "llama_perf_context_print:        eval time =      14.73 ms /     1 runs   (   14.73 ms per token,    67.90 tokens per second)\n",
            "llama_perf_context_print:       total time =      68.13 ms /    66 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 26 prefix-match hit, remaining 56 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      42.99 ms /    56 tokens (    0.77 ms per token,  1302.66 tokens per second)\n",
            "llama_perf_context_print:        eval time =      14.70 ms /     1 runs   (   14.70 ms per token,    68.01 tokens per second)\n",
            "llama_perf_context_print:       total time =      60.81 ms /    57 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 75 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      52.22 ms /    75 tokens (    0.70 ms per token,  1436.31 tokens per second)\n",
            "llama_perf_context_print:        eval time =      15.56 ms /     1 runs   (   15.56 ms per token,    64.25 tokens per second)\n",
            "llama_perf_context_print:       total time =      71.66 ms /    76 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 29 prefix-match hit, remaining 100 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      67.48 ms /   100 tokens (    0.67 ms per token,  1481.85 tokens per second)\n",
            "llama_perf_context_print:        eval time =      15.74 ms /     1 runs   (   15.74 ms per token,    63.55 tokens per second)\n",
            "llama_perf_context_print:       total time =      87.54 ms /   101 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 30 prefix-match hit, remaining 104 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      62.38 ms /   104 tokens (    0.60 ms per token,  1667.20 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.29 ms /     1 runs   (   16.29 ms per token,    61.38 tokens per second)\n",
            "llama_perf_context_print:       total time =      83.25 ms /   105 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 61 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      38.13 ms /    61 tokens (    0.63 ms per token,  1599.96 tokens per second)\n",
            "llama_perf_context_print:        eval time =      14.83 ms /     1 runs   (   14.83 ms per token,    67.44 tokens per second)\n",
            "llama_perf_context_print:       total time =      56.00 ms /    62 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 81 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      51.52 ms /    81 tokens (    0.64 ms per token,  1572.27 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.00 ms /     1 runs   (   16.00 ms per token,    62.49 tokens per second)\n",
            "llama_perf_context_print:       total time =      71.09 ms /    82 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 29 prefix-match hit, remaining 111 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      62.88 ms /   111 tokens (    0.57 ms per token,  1765.21 tokens per second)\n",
            "llama_perf_context_print:        eval time =      15.78 ms /     1 runs   (   15.78 ms per token,    63.37 tokens per second)\n",
            "llama_perf_context_print:       total time =      83.39 ms /   112 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 60 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      36.83 ms /    60 tokens (    0.61 ms per token,  1629.33 tokens per second)\n",
            "llama_perf_context_print:        eval time =      15.78 ms /     1 runs   (   15.78 ms per token,    63.38 tokens per second)\n",
            "llama_perf_context_print:       total time =      55.81 ms /    61 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 28 prefix-match hit, remaining 115 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      70.75 ms /   115 tokens (    0.62 ms per token,  1625.56 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.09 ms /     1 runs   (   16.09 ms per token,    62.16 tokens per second)\n",
            "llama_perf_context_print:       total time =      91.52 ms /   116 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 103 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      59.56 ms /   103 tokens (    0.58 ms per token,  1729.26 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.20 ms /     1 runs   (   16.20 ms per token,    61.73 tokens per second)\n",
            "llama_perf_context_print:       total time =      80.08 ms /   104 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 102 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      60.12 ms /   102 tokens (    0.59 ms per token,  1696.49 tokens per second)\n",
            "llama_perf_context_print:        eval time =      15.44 ms /     1 runs   (   15.44 ms per token,    64.77 tokens per second)\n",
            "llama_perf_context_print:       total time =      79.89 ms /   103 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 64 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      38.23 ms /    64 tokens (    0.60 ms per token,  1673.99 tokens per second)\n",
            "llama_perf_context_print:        eval time =      14.91 ms /     1 runs   (   14.91 ms per token,    67.05 tokens per second)\n",
            "llama_perf_context_print:       total time =      56.36 ms /    65 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 88 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      53.00 ms /    88 tokens (    0.60 ms per token,  1660.31 tokens per second)\n",
            "llama_perf_context_print:        eval time =      15.59 ms /     1 runs   (   15.59 ms per token,    64.16 tokens per second)\n",
            "llama_perf_context_print:       total time =      72.53 ms /    89 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 106 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      62.47 ms /   106 tokens (    0.59 ms per token,  1696.71 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.60 ms /     1 runs   (   16.60 ms per token,    60.24 tokens per second)\n",
            "llama_perf_context_print:       total time =      83.60 ms /   107 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 79 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      45.79 ms /    79 tokens (    0.58 ms per token,  1725.27 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.05 ms /     1 runs   (   16.05 ms per token,    62.32 tokens per second)\n",
            "llama_perf_context_print:       total time =      65.50 ms /    80 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 74 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      45.01 ms /    74 tokens (    0.61 ms per token,  1643.93 tokens per second)\n",
            "llama_perf_context_print:        eval time =      15.02 ms /     1 runs   (   15.02 ms per token,    66.59 tokens per second)\n",
            "llama_perf_context_print:       total time =      63.50 ms /    75 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 63 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      39.21 ms /    63 tokens (    0.62 ms per token,  1606.73 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.41 ms /     1 runs   (   16.41 ms per token,    60.93 tokens per second)\n",
            "llama_perf_context_print:       total time =      59.05 ms /    64 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 25 prefix-match hit, remaining 81 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      51.99 ms /    81 tokens (    0.64 ms per token,  1558.02 tokens per second)\n",
            "llama_perf_context_print:        eval time =      15.31 ms /     1 runs   (   15.31 ms per token,    65.33 tokens per second)\n",
            "llama_perf_context_print:       total time =      70.95 ms /    82 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 78 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      46.64 ms /    78 tokens (    0.60 ms per token,  1672.28 tokens per second)\n",
            "llama_perf_context_print:        eval time =      15.45 ms /     1 runs   (   15.45 ms per token,    64.73 tokens per second)\n",
            "llama_perf_context_print:       total time =      65.79 ms /    79 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 92 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      52.21 ms /    92 tokens (    0.57 ms per token,  1762.15 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.60 ms /     1 runs   (   16.60 ms per token,    60.23 tokens per second)\n",
            "llama_perf_context_print:       total time =      72.87 ms /    93 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 75 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      44.84 ms /    75 tokens (    0.60 ms per token,  1672.46 tokens per second)\n",
            "llama_perf_context_print:        eval time =      15.60 ms /     1 runs   (   15.60 ms per token,    64.09 tokens per second)\n",
            "llama_perf_context_print:       total time =      64.07 ms /    76 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 146 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      98.86 ms /   146 tokens (    0.68 ms per token,  1476.87 tokens per second)\n",
            "llama_perf_context_print:        eval time =      17.15 ms /     1 runs   (   17.15 ms per token,    58.30 tokens per second)\n",
            "llama_perf_context_print:       total time =     122.26 ms /   147 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 68 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      44.25 ms /    68 tokens (    0.65 ms per token,  1536.62 tokens per second)\n",
            "llama_perf_context_print:        eval time =      15.53 ms /     1 runs   (   15.53 ms per token,    64.40 tokens per second)\n",
            "llama_perf_context_print:       total time =      63.17 ms /    69 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 131 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      97.12 ms /   131 tokens (    0.74 ms per token,  1348.89 tokens per second)\n",
            "llama_perf_context_print:        eval time =      17.57 ms /     1 runs   (   17.57 ms per token,    56.91 tokens per second)\n",
            "llama_perf_context_print:       total time =     120.21 ms /   132 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 86 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      53.00 ms /    86 tokens (    0.62 ms per token,  1622.52 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.28 ms /     1 runs   (   16.28 ms per token,    61.42 tokens per second)\n",
            "llama_perf_context_print:       total time =      73.20 ms /    87 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 120 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      71.78 ms /   120 tokens (    0.60 ms per token,  1671.80 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.02 ms /     1 runs   (   16.02 ms per token,    62.41 tokens per second)\n",
            "llama_perf_context_print:       total time =      92.65 ms /   121 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 66 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      44.33 ms /    66 tokens (    0.67 ms per token,  1488.70 tokens per second)\n",
            "llama_perf_context_print:        eval time =      15.43 ms /     1 runs   (   15.43 ms per token,    64.80 tokens per second)\n",
            "llama_perf_context_print:       total time =      63.34 ms /    67 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 25 prefix-match hit, remaining 68 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      44.52 ms /    68 tokens (    0.65 ms per token,  1527.44 tokens per second)\n",
            "llama_perf_context_print:        eval time =      15.68 ms /     1 runs   (   15.68 ms per token,    63.77 tokens per second)\n",
            "llama_perf_context_print:       total time =      63.71 ms /    69 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 182 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =     118.40 ms /   182 tokens (    0.65 ms per token,  1537.15 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.91 ms /     1 runs   (   16.91 ms per token,    59.15 tokens per second)\n",
            "llama_perf_context_print:       total time =     141.80 ms /   183 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 108 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      62.73 ms /   108 tokens (    0.58 ms per token,  1721.58 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.61 ms /     1 runs   (   16.61 ms per token,    60.21 tokens per second)\n",
            "llama_perf_context_print:       total time =      83.93 ms /   109 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 63 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      38.13 ms /    63 tokens (    0.61 ms per token,  1652.42 tokens per second)\n",
            "llama_perf_context_print:        eval time =      15.43 ms /     1 runs   (   15.43 ms per token,    64.80 tokens per second)\n",
            "llama_perf_context_print:       total time =      56.78 ms /    64 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 115 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      70.39 ms /   115 tokens (    0.61 ms per token,  1633.85 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.34 ms /     1 runs   (   16.34 ms per token,    61.20 tokens per second)\n",
            "llama_perf_context_print:       total time =      91.40 ms /   116 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 98 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      62.54 ms /    98 tokens (    0.64 ms per token,  1567.02 tokens per second)\n",
            "llama_perf_context_print:        eval time =      15.98 ms /     1 runs   (   15.98 ms per token,    62.57 tokens per second)\n",
            "llama_perf_context_print:       total time =      82.68 ms /    99 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 25 prefix-match hit, remaining 101 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      61.21 ms /   101 tokens (    0.61 ms per token,  1650.14 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.81 ms /     1 runs   (   16.81 ms per token,    59.48 tokens per second)\n",
            "llama_perf_context_print:       total time =      82.22 ms /   102 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 83 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      52.05 ms /    83 tokens (    0.63 ms per token,  1594.68 tokens per second)\n",
            "llama_perf_context_print:        eval time =      15.78 ms /     1 runs   (   15.78 ms per token,    63.36 tokens per second)\n",
            "llama_perf_context_print:       total time =      71.54 ms /    84 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 105 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      61.95 ms /   105 tokens (    0.59 ms per token,  1695.02 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.24 ms /     1 runs   (   16.24 ms per token,    61.57 tokens per second)\n",
            "llama_perf_context_print:       total time =      82.62 ms /   106 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 60 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      40.36 ms /    60 tokens (    0.67 ms per token,  1486.69 tokens per second)\n",
            "llama_perf_context_print:        eval time =      14.63 ms /     1 runs   (   14.63 ms per token,    68.35 tokens per second)\n",
            "llama_perf_context_print:       total time =      58.07 ms /    61 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 97 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      62.40 ms /    97 tokens (    0.64 ms per token,  1554.56 tokens per second)\n",
            "llama_perf_context_print:        eval time =      15.75 ms /     1 runs   (   15.75 ms per token,    63.50 tokens per second)\n",
            "llama_perf_context_print:       total time =      82.39 ms /    98 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 134 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      98.34 ms /   134 tokens (    0.73 ms per token,  1362.61 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.32 ms /     1 runs   (   16.32 ms per token,    61.26 tokens per second)\n",
            "llama_perf_context_print:       total time =     119.94 ms /   135 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 66 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      43.44 ms /    66 tokens (    0.66 ms per token,  1519.44 tokens per second)\n",
            "llama_perf_context_print:        eval time =      15.31 ms /     1 runs   (   15.31 ms per token,    65.32 tokens per second)\n",
            "llama_perf_context_print:       total time =      62.01 ms /    67 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 143 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      99.75 ms /   143 tokens (    0.70 ms per token,  1433.63 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.43 ms /     1 runs   (   16.43 ms per token,    60.86 tokens per second)\n",
            "llama_perf_context_print:       total time =     121.74 ms /   144 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 102 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      61.44 ms /   102 tokens (    0.60 ms per token,  1660.10 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.64 ms /     1 runs   (   16.64 ms per token,    60.09 tokens per second)\n",
            "llama_perf_context_print:       total time =      82.51 ms /   103 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 59 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      40.65 ms /    59 tokens (    0.69 ms per token,  1451.34 tokens per second)\n",
            "llama_perf_context_print:        eval time =      14.74 ms /     1 runs   (   14.74 ms per token,    67.83 tokens per second)\n",
            "llama_perf_context_print:       total time =      58.53 ms /    60 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 67 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      44.38 ms /    67 tokens (    0.66 ms per token,  1509.83 tokens per second)\n",
            "llama_perf_context_print:        eval time =      15.59 ms /     1 runs   (   15.59 ms per token,    64.16 tokens per second)\n",
            "llama_perf_context_print:       total time =      63.25 ms /    68 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 106 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      64.63 ms /   106 tokens (    0.61 ms per token,  1640.11 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.34 ms /     1 runs   (   16.34 ms per token,    61.20 tokens per second)\n",
            "llama_perf_context_print:       total time =      85.55 ms /   107 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 25 prefix-match hit, remaining 81 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      52.37 ms /    81 tokens (    0.65 ms per token,  1546.69 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.06 ms /     1 runs   (   16.06 ms per token,    62.27 tokens per second)\n",
            "llama_perf_context_print:       total time =      72.15 ms /    82 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 66 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      44.24 ms /    66 tokens (    0.67 ms per token,  1491.86 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.85 ms /     1 runs   (   16.85 ms per token,    59.36 tokens per second)\n",
            "llama_perf_context_print:       total time =      64.25 ms /    67 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 99 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      61.54 ms /    99 tokens (    0.62 ms per token,  1608.74 tokens per second)\n",
            "llama_perf_context_print:        eval time =      15.99 ms /     1 runs   (   15.99 ms per token,    62.53 tokens per second)\n",
            "llama_perf_context_print:       total time =      81.80 ms /   100 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 84 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      51.50 ms /    84 tokens (    0.61 ms per token,  1631.19 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.97 ms /     1 runs   (   16.97 ms per token,    58.94 tokens per second)\n",
            "llama_perf_context_print:       total time =      72.37 ms /    85 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 83 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      50.51 ms /    83 tokens (    0.61 ms per token,  1643.34 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.87 ms /     1 runs   (   16.87 ms per token,    59.28 tokens per second)\n",
            "llama_perf_context_print:       total time =      71.23 ms /    84 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 80 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      45.60 ms /    80 tokens (    0.57 ms per token,  1754.35 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.14 ms /     1 runs   (   16.14 ms per token,    61.97 tokens per second)\n",
            "llama_perf_context_print:       total time =      66.24 ms /    81 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 51 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      37.68 ms /    51 tokens (    0.74 ms per token,  1353.47 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.18 ms /     1 runs   (   16.18 ms per token,    61.79 tokens per second)\n",
            "llama_perf_context_print:       total time =      56.93 ms /    52 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 67 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      46.13 ms /    67 tokens (    0.69 ms per token,  1452.51 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.15 ms /     1 runs   (   16.15 ms per token,    61.92 tokens per second)\n",
            "llama_perf_context_print:       total time =      65.54 ms /    68 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 78 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      47.05 ms /    78 tokens (    0.60 ms per token,  1657.63 tokens per second)\n",
            "llama_perf_context_print:        eval time =      17.29 ms /     1 runs   (   17.29 ms per token,    57.84 tokens per second)\n",
            "llama_perf_context_print:       total time =      67.94 ms /    79 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 96 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      54.27 ms /    96 tokens (    0.57 ms per token,  1768.84 tokens per second)\n",
            "llama_perf_context_print:        eval time =      18.70 ms /     1 runs   (   18.70 ms per token,    53.48 tokens per second)\n",
            "llama_perf_context_print:       total time =      77.16 ms /    97 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 82 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      52.70 ms /    82 tokens (    0.64 ms per token,  1555.83 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.02 ms /     1 runs   (   16.02 ms per token,    62.41 tokens per second)\n",
            "llama_perf_context_print:       total time =      72.54 ms /    83 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 25 prefix-match hit, remaining 51 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      37.21 ms /    51 tokens (    0.73 ms per token,  1370.71 tokens per second)\n",
            "llama_perf_context_print:        eval time =      15.87 ms /     1 runs   (   15.87 ms per token,    63.01 tokens per second)\n",
            "llama_perf_context_print:       total time =      56.13 ms /    52 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 100 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      69.76 ms /   100 tokens (    0.70 ms per token,  1433.47 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.03 ms /     1 runs   (   16.03 ms per token,    62.40 tokens per second)\n",
            "llama_perf_context_print:       total time =      90.77 ms /   101 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 76 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      54.81 ms /    76 tokens (    0.72 ms per token,  1386.66 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.57 ms /     1 runs   (   16.57 ms per token,    60.34 tokens per second)\n",
            "llama_perf_context_print:       total time =      77.23 ms /    77 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 88 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      61.53 ms /    88 tokens (    0.70 ms per token,  1430.10 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.20 ms /     1 runs   (   16.20 ms per token,    61.72 tokens per second)\n",
            "llama_perf_context_print:       total time =      85.19 ms /    89 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 81 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      60.49 ms /    81 tokens (    0.75 ms per token,  1339.02 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.30 ms /     1 runs   (   16.30 ms per token,    61.35 tokens per second)\n",
            "llama_perf_context_print:       total time =      80.67 ms /    82 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 97 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      68.93 ms /    97 tokens (    0.71 ms per token,  1407.18 tokens per second)\n",
            "llama_perf_context_print:        eval time =      15.45 ms /     1 runs   (   15.45 ms per token,    64.75 tokens per second)\n",
            "llama_perf_context_print:       total time =      88.26 ms /    98 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 30 prefix-match hit, remaining 95 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      57.98 ms /    95 tokens (    0.61 ms per token,  1638.50 tokens per second)\n",
            "llama_perf_context_print:        eval time =      15.77 ms /     1 runs   (   15.77 ms per token,    63.42 tokens per second)\n",
            "llama_perf_context_print:       total time =      77.89 ms /    96 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 59 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      40.81 ms /    59 tokens (    0.69 ms per token,  1445.65 tokens per second)\n",
            "llama_perf_context_print:        eval time =      14.57 ms /     1 runs   (   14.57 ms per token,    68.65 tokens per second)\n",
            "llama_perf_context_print:       total time =      58.46 ms /    60 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 101 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      63.43 ms /   101 tokens (    0.63 ms per token,  1592.41 tokens per second)\n",
            "llama_perf_context_print:        eval time =      15.80 ms /     1 runs   (   15.80 ms per token,    63.28 tokens per second)\n",
            "llama_perf_context_print:       total time =      83.40 ms /   102 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 141 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =     100.63 ms /   141 tokens (    0.71 ms per token,  1401.23 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.68 ms /     1 runs   (   16.68 ms per token,    59.93 tokens per second)\n",
            "llama_perf_context_print:       total time =     122.78 ms /   142 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 25 prefix-match hit, remaining 62 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      38.04 ms /    62 tokens (    0.61 ms per token,  1629.82 tokens per second)\n",
            "llama_perf_context_print:        eval time =      15.48 ms /     1 runs   (   15.48 ms per token,    64.60 tokens per second)\n",
            "llama_perf_context_print:       total time =      56.61 ms /    63 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 63 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      38.26 ms /    63 tokens (    0.61 ms per token,  1646.50 tokens per second)\n",
            "llama_perf_context_print:        eval time =      15.38 ms /     1 runs   (   15.38 ms per token,    65.03 tokens per second)\n",
            "llama_perf_context_print:       total time =      56.85 ms /    64 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 66 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      43.72 ms /    66 tokens (    0.66 ms per token,  1509.50 tokens per second)\n",
            "llama_perf_context_print:        eval time =      15.72 ms /     1 runs   (   15.72 ms per token,    63.60 tokens per second)\n",
            "llama_perf_context_print:       total time =      62.95 ms /    67 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 98 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      59.94 ms /    98 tokens (    0.61 ms per token,  1634.86 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.20 ms /     1 runs   (   16.20 ms per token,    61.72 tokens per second)\n",
            "llama_perf_context_print:       total time =      80.58 ms /    99 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 80 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      45.72 ms /    80 tokens (    0.57 ms per token,  1749.67 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.33 ms /     1 runs   (   16.33 ms per token,    61.23 tokens per second)\n",
            "llama_perf_context_print:       total time =      65.72 ms /    81 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 82 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      50.80 ms /    82 tokens (    0.62 ms per token,  1614.01 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.87 ms /     1 runs   (   16.87 ms per token,    59.29 tokens per second)\n",
            "llama_perf_context_print:       total time =      71.42 ms /    83 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 25 prefix-match hit, remaining 105 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      64.86 ms /   105 tokens (    0.62 ms per token,  1618.87 tokens per second)\n",
            "llama_perf_context_print:        eval time =      15.60 ms /     1 runs   (   15.60 ms per token,    64.09 tokens per second)\n",
            "llama_perf_context_print:       total time =      84.57 ms /   106 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 61 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      38.44 ms /    61 tokens (    0.63 ms per token,  1586.72 tokens per second)\n",
            "llama_perf_context_print:        eval time =      15.91 ms /     1 runs   (   15.91 ms per token,    62.87 tokens per second)\n",
            "llama_perf_context_print:       total time =      57.50 ms /    62 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 80 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      46.88 ms /    80 tokens (    0.59 ms per token,  1706.45 tokens per second)\n",
            "llama_perf_context_print:        eval time =      15.57 ms /     1 runs   (   15.57 ms per token,    64.21 tokens per second)\n",
            "llama_perf_context_print:       total time =      66.19 ms /    81 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 113 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      71.39 ms /   113 tokens (    0.63 ms per token,  1582.81 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.60 ms /     1 runs   (   16.60 ms per token,    60.23 tokens per second)\n",
            "llama_perf_context_print:       total time =      92.53 ms /   114 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 90 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      56.79 ms /    90 tokens (    0.63 ms per token,  1584.73 tokens per second)\n",
            "llama_perf_context_print:        eval time =      15.64 ms /     1 runs   (   15.64 ms per token,    63.95 tokens per second)\n",
            "llama_perf_context_print:       total time =      76.50 ms /    91 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 29 prefix-match hit, remaining 74 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      49.22 ms /    74 tokens (    0.67 ms per token,  1503.55 tokens per second)\n",
            "llama_perf_context_print:        eval time =      15.12 ms /     1 runs   (   15.12 ms per token,    66.16 tokens per second)\n",
            "llama_perf_context_print:       total time =      68.46 ms /    75 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 109 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      65.71 ms /   109 tokens (    0.60 ms per token,  1658.83 tokens per second)\n",
            "llama_perf_context_print:        eval time =      15.96 ms /     1 runs   (   15.96 ms per token,    62.67 tokens per second)\n",
            "llama_perf_context_print:       total time =      86.27 ms /   110 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 99 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      59.46 ms /    99 tokens (    0.60 ms per token,  1664.93 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.62 ms /     1 runs   (   16.62 ms per token,    60.17 tokens per second)\n",
            "llama_perf_context_print:       total time =      80.44 ms /   100 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 102 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      61.88 ms /   102 tokens (    0.61 ms per token,  1648.27 tokens per second)\n",
            "llama_perf_context_print:        eval time =      84.05 ms /     5 runs   (   16.81 ms per token,    59.49 tokens per second)\n",
            "llama_perf_context_print:       total time =     152.46 ms /   107 tokens\n",
            "llama_perf_context_print:    graphs reused =          3\n",
            "Llama.generate: 24 prefix-match hit, remaining 63 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      37.40 ms /    63 tokens (    0.59 ms per token,  1684.36 tokens per second)\n",
            "llama_perf_context_print:        eval time =      14.99 ms /     1 runs   (   14.99 ms per token,    66.72 tokens per second)\n",
            "llama_perf_context_print:       total time =      55.70 ms /    64 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 163 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =     114.47 ms /   163 tokens (    0.70 ms per token,  1424.00 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.16 ms /     1 runs   (   16.16 ms per token,    61.89 tokens per second)\n",
            "llama_perf_context_print:       total time =     136.61 ms /   164 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 85 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      51.66 ms /    85 tokens (    0.61 ms per token,  1645.28 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.59 ms /     1 runs   (   16.59 ms per token,    60.27 tokens per second)\n",
            "llama_perf_context_print:       total time =      72.21 ms /    86 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 64 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      38.11 ms /    64 tokens (    0.60 ms per token,  1679.53 tokens per second)\n",
            "llama_perf_context_print:        eval time =      82.32 ms /     5 runs   (   16.46 ms per token,    60.74 tokens per second)\n",
            "llama_perf_context_print:       total time =     125.55 ms /    69 tokens\n",
            "llama_perf_context_print:    graphs reused =          4\n",
            "Llama.generate: 24 prefix-match hit, remaining 128 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      72.98 ms /   128 tokens (    0.57 ms per token,  1753.79 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.79 ms /     1 runs   (   16.79 ms per token,    59.58 tokens per second)\n",
            "llama_perf_context_print:       total time =      94.88 ms /   129 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 92 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      52.60 ms /    92 tokens (    0.57 ms per token,  1748.92 tokens per second)\n",
            "llama_perf_context_print:        eval time =      17.07 ms /     1 runs   (   17.07 ms per token,    58.60 tokens per second)\n",
            "llama_perf_context_print:       total time =      73.87 ms /    93 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 135 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      97.29 ms /   135 tokens (    0.72 ms per token,  1387.65 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.86 ms /     1 runs   (   16.86 ms per token,    59.33 tokens per second)\n",
            "llama_perf_context_print:       total time =     119.50 ms /   136 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 69 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      45.07 ms /    69 tokens (    0.65 ms per token,  1530.82 tokens per second)\n",
            "llama_perf_context_print:        eval time =      15.77 ms /     1 runs   (   15.77 ms per token,    63.41 tokens per second)\n",
            "llama_perf_context_print:       total time =      64.27 ms /    70 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 28 prefix-match hit, remaining 65 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      45.57 ms /    65 tokens (    0.70 ms per token,  1426.47 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.59 ms /     1 runs   (   16.59 ms per token,    60.27 tokens per second)\n",
            "llama_perf_context_print:       total time =      65.83 ms /    66 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 28 prefix-match hit, remaining 97 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      60.95 ms /    97 tokens (    0.63 ms per token,  1591.42 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.01 ms /     1 runs   (   16.01 ms per token,    62.46 tokens per second)\n",
            "llama_perf_context_print:       total time =      81.27 ms /    98 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 28 prefix-match hit, remaining 85 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      53.78 ms /    85 tokens (    0.63 ms per token,  1580.60 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.31 ms /     1 runs   (   16.31 ms per token,    61.30 tokens per second)\n",
            "llama_perf_context_print:       total time =      73.76 ms /    86 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 95 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      55.74 ms /    95 tokens (    0.59 ms per token,  1704.43 tokens per second)\n",
            "llama_perf_context_print:        eval time =      15.75 ms /     1 runs   (   15.75 ms per token,    63.48 tokens per second)\n",
            "llama_perf_context_print:       total time =      75.66 ms /    96 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 82 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      53.06 ms /    82 tokens (    0.65 ms per token,  1545.42 tokens per second)\n",
            "llama_perf_context_print:        eval time =      15.32 ms /     1 runs   (   15.32 ms per token,    65.26 tokens per second)\n",
            "llama_perf_context_print:       total time =      72.13 ms /    83 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 84 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      52.33 ms /    84 tokens (    0.62 ms per token,  1605.11 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.21 ms /     1 runs   (   16.21 ms per token,    61.70 tokens per second)\n",
            "llama_perf_context_print:       total time =      72.21 ms /    85 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 25 prefix-match hit, remaining 128 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      73.83 ms /   128 tokens (    0.58 ms per token,  1733.71 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.01 ms /     1 runs   (   16.01 ms per token,    62.46 tokens per second)\n",
            "llama_perf_context_print:       total time =      94.75 ms /   129 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 74 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      48.52 ms /    74 tokens (    0.66 ms per token,  1525.08 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      51.92 ms /    75 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 225 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =     150.96 ms /   225 tokens (    0.67 ms per token,  1490.43 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =     158.42 ms /   226 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 51 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      35.82 ms /    51 tokens (    0.70 ms per token,  1423.67 tokens per second)\n",
            "llama_perf_context_print:        eval time =      15.23 ms /     1 runs   (   15.23 ms per token,    65.64 tokens per second)\n",
            "llama_perf_context_print:       total time =      53.95 ms /    52 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 180 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =     118.97 ms /   180 tokens (    0.66 ms per token,  1513.00 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.97 ms /     1 runs   (   16.97 ms per token,    58.93 tokens per second)\n",
            "llama_perf_context_print:       total time =     142.03 ms /   181 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 25 prefix-match hit, remaining 105 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      61.67 ms /   105 tokens (    0.59 ms per token,  1702.53 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      65.60 ms /   106 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 103 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      61.23 ms /   103 tokens (    0.59 ms per token,  1682.10 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.29 ms /     1 runs   (   16.29 ms per token,    61.39 tokens per second)\n",
            "llama_perf_context_print:       total time =      83.43 ms /   104 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 189 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =     118.78 ms /   189 tokens (    0.63 ms per token,  1591.20 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.12 ms /     1 runs   (   16.12 ms per token,    62.05 tokens per second)\n",
            "llama_perf_context_print:       total time =     141.27 ms /   190 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 116 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      74.09 ms /   116 tokens (    0.64 ms per token,  1565.75 tokens per second)\n",
            "llama_perf_context_print:        eval time =      15.70 ms /     1 runs   (   15.70 ms per token,    63.69 tokens per second)\n",
            "llama_perf_context_print:       total time =      94.60 ms /   117 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 82 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      55.18 ms /    82 tokens (    0.67 ms per token,  1486.15 tokens per second)\n",
            "llama_perf_context_print:        eval time =      14.93 ms /     1 runs   (   14.93 ms per token,    66.96 tokens per second)\n",
            "llama_perf_context_print:       total time =      73.82 ms /    83 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 119 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      73.91 ms /   119 tokens (    0.62 ms per token,  1609.98 tokens per second)\n",
            "llama_perf_context_print:        eval time =      15.21 ms /     1 runs   (   15.21 ms per token,    65.73 tokens per second)\n",
            "llama_perf_context_print:       total time =      93.91 ms /   120 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 104 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      60.08 ms /   104 tokens (    0.58 ms per token,  1730.97 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.31 ms /     1 runs   (   16.31 ms per token,    61.31 tokens per second)\n",
            "llama_perf_context_print:       total time =      80.76 ms /   105 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 104 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      60.35 ms /   104 tokens (    0.58 ms per token,  1723.20 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.24 ms /     1 runs   (   16.24 ms per token,    61.58 tokens per second)\n",
            "llama_perf_context_print:       total time =      80.97 ms /   105 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 144 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      99.03 ms /   144 tokens (    0.69 ms per token,  1454.05 tokens per second)\n",
            "llama_perf_context_print:        eval time =      17.49 ms /     1 runs   (   17.49 ms per token,    57.18 tokens per second)\n",
            "llama_perf_context_print:       total time =     122.36 ms /   145 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 159 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =     101.39 ms /   159 tokens (    0.64 ms per token,  1568.28 tokens per second)\n",
            "llama_perf_context_print:        eval time =      17.71 ms /     1 runs   (   17.71 ms per token,    56.46 tokens per second)\n",
            "llama_perf_context_print:       total time =     125.36 ms /   160 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 117 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      72.20 ms /   117 tokens (    0.62 ms per token,  1620.52 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.07 ms /     1 runs   (   16.07 ms per token,    62.25 tokens per second)\n",
            "llama_perf_context_print:       total time =      93.12 ms /   118 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 101 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      63.39 ms /   101 tokens (    0.63 ms per token,  1593.36 tokens per second)\n",
            "llama_perf_context_print:        eval time =      15.01 ms /     1 runs   (   15.01 ms per token,    66.64 tokens per second)\n",
            "llama_perf_context_print:       total time =      82.66 ms /   102 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 133 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      99.29 ms /   133 tokens (    0.75 ms per token,  1339.55 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.42 ms /     1 runs   (   16.42 ms per token,    60.91 tokens per second)\n",
            "llama_perf_context_print:       total time =     120.93 ms /   134 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 97 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      61.96 ms /    97 tokens (    0.64 ms per token,  1565.60 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.00 ms /     1 runs   (   16.00 ms per token,    62.50 tokens per second)\n",
            "llama_perf_context_print:       total time =      82.22 ms /    98 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 106 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      66.85 ms /   106 tokens (    0.63 ms per token,  1585.73 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      71.23 ms /   107 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 185 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =     119.44 ms /   185 tokens (    0.65 ms per token,  1548.87 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.63 ms /     1 runs   (   16.63 ms per token,    60.12 tokens per second)\n",
            "llama_perf_context_print:       total time =     143.10 ms /   186 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 113 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      70.74 ms /   113 tokens (    0.63 ms per token,  1597.33 tokens per second)\n",
            "llama_perf_context_print:        eval time =      17.10 ms /     1 runs   (   17.10 ms per token,    58.47 tokens per second)\n",
            "llama_perf_context_print:       total time =      92.77 ms /   114 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 63 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      42.63 ms /    63 tokens (    0.68 ms per token,  1477.97 tokens per second)\n",
            "llama_perf_context_print:        eval time =      15.91 ms /     1 runs   (   15.91 ms per token,    62.87 tokens per second)\n",
            "llama_perf_context_print:       total time =      62.41 ms /    64 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 112 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      67.28 ms /   112 tokens (    0.60 ms per token,  1664.71 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      71.63 ms /   113 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 104 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      64.45 ms /   104 tokens (    0.62 ms per token,  1613.53 tokens per second)\n",
            "llama_perf_context_print:        eval time =      15.88 ms /     1 runs   (   15.88 ms per token,    62.96 tokens per second)\n",
            "llama_perf_context_print:       total time =      84.76 ms /   105 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 83 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      54.84 ms /    83 tokens (    0.66 ms per token,  1513.52 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.51 ms /     1 runs   (   16.51 ms per token,    60.55 tokens per second)\n",
            "llama_perf_context_print:       total time =      75.22 ms /    84 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 74 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      48.64 ms /    74 tokens (    0.66 ms per token,  1521.51 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.79 ms /     1 runs   (   16.79 ms per token,    59.57 tokens per second)\n",
            "llama_perf_context_print:       total time =      69.27 ms /    75 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 25 prefix-match hit, remaining 80 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      49.97 ms /    80 tokens (    0.62 ms per token,  1601.09 tokens per second)\n",
            "llama_perf_context_print:        eval time =      15.28 ms /     1 runs   (   15.28 ms per token,    65.45 tokens per second)\n",
            "llama_perf_context_print:       total time =      74.33 ms /    81 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 177 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =     117.91 ms /   177 tokens (    0.67 ms per token,  1501.18 tokens per second)\n",
            "llama_perf_context_print:        eval time =      17.15 ms /     1 runs   (   17.15 ms per token,    58.32 tokens per second)\n",
            "llama_perf_context_print:       total time =     141.58 ms /   178 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 200 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =     132.92 ms /   200 tokens (    0.66 ms per token,  1504.64 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.28 ms /     1 runs   (   16.28 ms per token,    61.41 tokens per second)\n",
            "llama_perf_context_print:       total time =     156.58 ms /   201 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 56 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      35.75 ms /    56 tokens (    0.64 ms per token,  1566.48 tokens per second)\n",
            "llama_perf_context_print:        eval time =      14.72 ms /     1 runs   (   14.72 ms per token,    67.93 tokens per second)\n",
            "llama_perf_context_print:       total time =      53.56 ms /    57 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 75 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      45.18 ms /    75 tokens (    0.60 ms per token,  1660.06 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.38 ms /     1 runs   (   16.38 ms per token,    61.05 tokens per second)\n",
            "llama_perf_context_print:       total time =      65.50 ms /    76 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 95 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      52.95 ms /    95 tokens (    0.56 ms per token,  1794.08 tokens per second)\n",
            "llama_perf_context_print:        eval time =      17.03 ms /     1 runs   (   17.03 ms per token,    58.71 tokens per second)\n",
            "llama_perf_context_print:       total time =      75.70 ms /    96 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 90 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      51.97 ms /    90 tokens (    0.58 ms per token,  1731.90 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.26 ms /     1 runs   (   16.26 ms per token,    61.49 tokens per second)\n",
            "llama_perf_context_print:       total time =      72.22 ms /    91 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 28 prefix-match hit, remaining 98 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      59.59 ms /    98 tokens (    0.61 ms per token,  1644.54 tokens per second)\n",
            "llama_perf_context_print:        eval time =      15.58 ms /     1 runs   (   15.58 ms per token,    64.17 tokens per second)\n",
            "llama_perf_context_print:       total time =      79.52 ms /    99 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 81 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      54.56 ms /    81 tokens (    0.67 ms per token,  1484.60 tokens per second)\n",
            "llama_perf_context_print:        eval time =      15.50 ms /     1 runs   (   15.50 ms per token,    64.50 tokens per second)\n",
            "llama_perf_context_print:       total time =      73.79 ms /    82 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 206 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =     134.67 ms /   206 tokens (    0.65 ms per token,  1529.65 tokens per second)\n",
            "llama_perf_context_print:        eval time =      15.98 ms /     1 runs   (   15.98 ms per token,    62.57 tokens per second)\n",
            "llama_perf_context_print:       total time =     157.52 ms /   207 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 25 prefix-match hit, remaining 115 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      70.94 ms /   115 tokens (    0.62 ms per token,  1621.07 tokens per second)\n",
            "llama_perf_context_print:        eval time =      15.70 ms /     1 runs   (   15.70 ms per token,    63.70 tokens per second)\n",
            "llama_perf_context_print:       total time =      91.30 ms /   116 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 79 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      48.37 ms /    79 tokens (    0.61 ms per token,  1633.28 tokens per second)\n",
            "llama_perf_context_print:        eval time =      15.79 ms /     1 runs   (   15.79 ms per token,    63.32 tokens per second)\n",
            "llama_perf_context_print:       total time =      67.87 ms /    80 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 177 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =     116.29 ms /   177 tokens (    0.66 ms per token,  1522.00 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.37 ms /     1 runs   (   16.37 ms per token,    61.09 tokens per second)\n",
            "llama_perf_context_print:       total time =     138.61 ms /   178 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 98 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      60.16 ms /    98 tokens (    0.61 ms per token,  1628.96 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.50 ms /     1 runs   (   16.50 ms per token,    60.60 tokens per second)\n",
            "llama_perf_context_print:       total time =      80.93 ms /    99 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 157 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =     101.10 ms /   157 tokens (    0.64 ms per token,  1552.92 tokens per second)\n",
            "llama_perf_context_print:        eval time =      17.21 ms /     1 runs   (   17.21 ms per token,    58.12 tokens per second)\n",
            "llama_perf_context_print:       total time =     124.31 ms /   158 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 77 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      45.79 ms /    77 tokens (    0.59 ms per token,  1681.77 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.59 ms /     1 runs   (   16.59 ms per token,    60.29 tokens per second)\n",
            "llama_perf_context_print:       total time =      66.40 ms /    78 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 82 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      51.12 ms /    82 tokens (    0.62 ms per token,  1604.13 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.21 ms /     1 runs   (   16.21 ms per token,    61.71 tokens per second)\n",
            "llama_perf_context_print:       total time =      71.17 ms /    83 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 25 prefix-match hit, remaining 75 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      45.29 ms /    75 tokens (    0.60 ms per token,  1656.03 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      48.37 ms /    76 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 167 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =     114.13 ms /   167 tokens (    0.68 ms per token,  1463.24 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.84 ms /     1 runs   (   16.84 ms per token,    59.38 tokens per second)\n",
            "llama_perf_context_print:       total time =     137.04 ms /   168 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 56 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      36.72 ms /    56 tokens (    0.66 ms per token,  1525.22 tokens per second)\n",
            "llama_perf_context_print:        eval time =      14.80 ms /     1 runs   (   14.80 ms per token,    67.54 tokens per second)\n",
            "llama_perf_context_print:       total time =      54.64 ms /    57 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 89 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      52.87 ms /    89 tokens (    0.59 ms per token,  1683.31 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      56.58 ms /    90 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 84 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      52.44 ms /    84 tokens (    0.62 ms per token,  1601.77 tokens per second)\n",
            "llama_perf_context_print:        eval time =      15.00 ms /     1 runs   (   15.00 ms per token,    66.68 tokens per second)\n",
            "llama_perf_context_print:       total time =      71.42 ms /    85 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 25 prefix-match hit, remaining 127 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      74.13 ms /   127 tokens (    0.58 ms per token,  1713.16 tokens per second)\n",
            "llama_perf_context_print:        eval time =      15.86 ms /     1 runs   (   15.86 ms per token,    63.06 tokens per second)\n",
            "llama_perf_context_print:       total time =      95.16 ms /   128 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 115 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      71.74 ms /   115 tokens (    0.62 ms per token,  1603.06 tokens per second)\n",
            "llama_perf_context_print:        eval time =     115.67 ms /     7 runs   (   16.52 ms per token,    60.51 tokens per second)\n",
            "llama_perf_context_print:       total time =     195.12 ms /   122 tokens\n",
            "llama_perf_context_print:    graphs reused =          6\n",
            "Llama.generate: 24 prefix-match hit, remaining 137 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      96.13 ms /   137 tokens (    0.70 ms per token,  1425.15 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.84 ms /     1 runs   (   16.84 ms per token,    59.38 tokens per second)\n",
            "llama_perf_context_print:       total time =     118.34 ms /   138 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 98 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      57.71 ms /    98 tokens (    0.59 ms per token,  1698.03 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.24 ms /     1 runs   (   16.24 ms per token,    61.57 tokens per second)\n",
            "llama_perf_context_print:       total time =      78.41 ms /    99 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 98 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      60.19 ms /    98 tokens (    0.61 ms per token,  1628.07 tokens per second)\n",
            "llama_perf_context_print:        eval time =      15.66 ms /     1 runs   (   15.66 ms per token,    63.85 tokens per second)\n",
            "llama_perf_context_print:       total time =      80.13 ms /    99 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 64 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      39.40 ms /    64 tokens (    0.62 ms per token,  1624.53 tokens per second)\n",
            "llama_perf_context_print:        eval time =      15.44 ms /     1 runs   (   15.44 ms per token,    64.75 tokens per second)\n",
            "llama_perf_context_print:       total time =      58.13 ms /    65 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 70 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      46.47 ms /    70 tokens (    0.66 ms per token,  1506.32 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.49 ms /     1 runs   (   16.49 ms per token,    60.65 tokens per second)\n",
            "llama_perf_context_print:       total time =      67.13 ms /    71 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 114 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      71.67 ms /   114 tokens (    0.63 ms per token,  1590.71 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      75.68 ms /   115 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 131 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      94.22 ms /   131 tokens (    0.72 ms per token,  1390.36 tokens per second)\n",
            "llama_perf_context_print:        eval time =      17.49 ms /     1 runs   (   17.49 ms per token,    57.19 tokens per second)\n",
            "llama_perf_context_print:       total time =     117.43 ms /   132 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 124 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      72.96 ms /   124 tokens (    0.59 ms per token,  1699.65 tokens per second)\n",
            "llama_perf_context_print:        eval time =      81.94 ms /     5 runs   (   16.39 ms per token,    61.02 tokens per second)\n",
            "llama_perf_context_print:       total time =     162.14 ms /   129 tokens\n",
            "llama_perf_context_print:    graphs reused =          4\n",
            "Llama.generate: 24 prefix-match hit, remaining 141 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      96.83 ms /   141 tokens (    0.69 ms per token,  1456.10 tokens per second)\n",
            "llama_perf_context_print:        eval time =      17.34 ms /     1 runs   (   17.34 ms per token,    57.67 tokens per second)\n",
            "llama_perf_context_print:       total time =     119.93 ms /   142 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 114 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      70.54 ms /   114 tokens (    0.62 ms per token,  1616.04 tokens per second)\n",
            "llama_perf_context_print:        eval time =      81.06 ms /     5 runs   (   16.21 ms per token,    61.68 tokens per second)\n",
            "llama_perf_context_print:       total time =     158.36 ms /   119 tokens\n",
            "llama_perf_context_print:    graphs reused =          4\n",
            "Llama.generate: 24 prefix-match hit, remaining 81 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      49.64 ms /    81 tokens (    0.61 ms per token,  1631.88 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.83 ms /     1 runs   (   16.83 ms per token,    59.43 tokens per second)\n",
            "llama_perf_context_print:       total time =      70.27 ms /    82 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 253 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =     160.25 ms /   253 tokens (    0.63 ms per token,  1578.81 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.38 ms /     1 runs   (   16.38 ms per token,    61.04 tokens per second)\n",
            "llama_perf_context_print:       total time =     185.41 ms /   254 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 143 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      97.18 ms /   143 tokens (    0.68 ms per token,  1471.45 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.51 ms /     1 runs   (   16.51 ms per token,    60.58 tokens per second)\n",
            "llama_perf_context_print:       total time =     119.40 ms /   144 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 189 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =     117.97 ms /   189 tokens (    0.62 ms per token,  1602.14 tokens per second)\n",
            "llama_perf_context_print:        eval time =      15.95 ms /     1 runs   (   15.95 ms per token,    62.70 tokens per second)\n",
            "llama_perf_context_print:       total time =     140.39 ms /   190 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 79 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      45.69 ms /    79 tokens (    0.58 ms per token,  1729.01 tokens per second)\n",
            "llama_perf_context_print:        eval time =      15.61 ms /     1 runs   (   15.61 ms per token,    64.05 tokens per second)\n",
            "llama_perf_context_print:       total time =      65.03 ms /    80 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 175 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =     115.73 ms /   175 tokens (    0.66 ms per token,  1512.09 tokens per second)\n",
            "llama_perf_context_print:        eval time =      15.95 ms /     1 runs   (   15.95 ms per token,    62.72 tokens per second)\n",
            "llama_perf_context_print:       total time =     137.80 ms /   176 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 77 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      49.66 ms /    77 tokens (    0.64 ms per token,  1550.54 tokens per second)\n",
            "llama_perf_context_print:        eval time =      15.32 ms /     1 runs   (   15.32 ms per token,    65.27 tokens per second)\n",
            "llama_perf_context_print:       total time =      68.73 ms /    78 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 61 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      43.33 ms /    61 tokens (    0.71 ms per token,  1407.90 tokens per second)\n",
            "llama_perf_context_print:        eval time =      14.60 ms /     1 runs   (   14.60 ms per token,    68.49 tokens per second)\n",
            "llama_perf_context_print:       total time =      61.02 ms /    62 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 101 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      62.99 ms /   101 tokens (    0.62 ms per token,  1603.43 tokens per second)\n",
            "llama_perf_context_print:        eval time =      79.56 ms /     5 runs   (   15.91 ms per token,    62.84 tokens per second)\n",
            "llama_perf_context_print:       total time =     149.37 ms /   106 tokens\n",
            "llama_perf_context_print:    graphs reused =          3\n",
            "Llama.generate: 24 prefix-match hit, remaining 157 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =     100.22 ms /   157 tokens (    0.64 ms per token,  1566.54 tokens per second)\n",
            "llama_perf_context_print:        eval time =      17.03 ms /     1 runs   (   17.03 ms per token,    58.71 tokens per second)\n",
            "llama_perf_context_print:       total time =     123.29 ms /   158 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 90 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      52.69 ms /    90 tokens (    0.59 ms per token,  1708.14 tokens per second)\n",
            "llama_perf_context_print:        eval time =      15.74 ms /     1 runs   (   15.74 ms per token,    63.53 tokens per second)\n",
            "llama_perf_context_print:       total time =      72.42 ms /    91 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 57 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      37.31 ms /    57 tokens (    0.65 ms per token,  1527.54 tokens per second)\n",
            "llama_perf_context_print:        eval time =      14.68 ms /     1 runs   (   14.68 ms per token,    68.12 tokens per second)\n",
            "llama_perf_context_print:       total time =      54.94 ms /    58 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 80 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      45.80 ms /    80 tokens (    0.57 ms per token,  1746.76 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.08 ms /     1 runs   (   16.08 ms per token,    62.18 tokens per second)\n",
            "llama_perf_context_print:       total time =      65.66 ms /    81 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 61 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      37.24 ms /    61 tokens (    0.61 ms per token,  1638.07 tokens per second)\n",
            "llama_perf_context_print:        eval time =      14.75 ms /     1 runs   (   14.75 ms per token,    67.78 tokens per second)\n",
            "llama_perf_context_print:       total time =      55.25 ms /    62 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 76 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      45.53 ms /    76 tokens (    0.60 ms per token,  1669.27 tokens per second)\n",
            "llama_perf_context_print:        eval time =      15.58 ms /     1 runs   (   15.58 ms per token,    64.19 tokens per second)\n",
            "llama_perf_context_print:       total time =      64.90 ms /    77 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 57 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      37.93 ms /    57 tokens (    0.67 ms per token,  1502.81 tokens per second)\n",
            "llama_perf_context_print:        eval time =      14.48 ms /     1 runs   (   14.48 ms per token,    69.07 tokens per second)\n",
            "llama_perf_context_print:       total time =      55.54 ms /    58 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 99 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      60.03 ms /    99 tokens (    0.61 ms per token,  1649.15 tokens per second)\n",
            "llama_perf_context_print:        eval time =      15.18 ms /     1 runs   (   15.18 ms per token,    65.86 tokens per second)\n",
            "llama_perf_context_print:       total time =      79.48 ms /   100 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 58 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      38.62 ms /    58 tokens (    0.67 ms per token,  1501.97 tokens per second)\n",
            "llama_perf_context_print:        eval time =      15.64 ms /     1 runs   (   15.64 ms per token,    63.95 tokens per second)\n",
            "llama_perf_context_print:       total time =      57.38 ms /    59 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 55 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      37.40 ms /    55 tokens (    0.68 ms per token,  1470.78 tokens per second)\n",
            "llama_perf_context_print:        eval time =      15.03 ms /     1 runs   (   15.03 ms per token,    66.53 tokens per second)\n",
            "llama_perf_context_print:       total time =      55.76 ms /    56 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 41 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      30.62 ms /    41 tokens (    0.75 ms per token,  1339.04 tokens per second)\n",
            "llama_perf_context_print:        eval time =      14.89 ms /     1 runs   (   14.89 ms per token,    67.14 tokens per second)\n",
            "llama_perf_context_print:       total time =      48.29 ms /    42 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 59 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      37.68 ms /    59 tokens (    0.64 ms per token,  1565.82 tokens per second)\n",
            "llama_perf_context_print:        eval time =      15.28 ms /     1 runs   (   15.28 ms per token,    65.46 tokens per second)\n",
            "llama_perf_context_print:       total time =      56.34 ms /    60 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 75 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      45.79 ms /    75 tokens (    0.61 ms per token,  1638.06 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.47 ms /     1 runs   (   16.47 ms per token,    60.71 tokens per second)\n",
            "llama_perf_context_print:       total time =      67.52 ms /    76 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 58 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      38.26 ms /    58 tokens (    0.66 ms per token,  1515.98 tokens per second)\n",
            "llama_perf_context_print:        eval time =      15.19 ms /     1 runs   (   15.19 ms per token,    65.82 tokens per second)\n",
            "llama_perf_context_print:       total time =      56.63 ms /    59 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 77 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      46.22 ms /    77 tokens (    0.60 ms per token,  1665.84 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.41 ms /     1 runs   (   16.41 ms per token,    60.96 tokens per second)\n",
            "llama_perf_context_print:       total time =      66.17 ms /    78 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 58 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      43.86 ms /    58 tokens (    0.76 ms per token,  1322.45 tokens per second)\n",
            "llama_perf_context_print:        eval time =      15.24 ms /     1 runs   (   15.24 ms per token,    65.62 tokens per second)\n",
            "llama_perf_context_print:       total time =      62.40 ms /    59 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 56 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      43.66 ms /    56 tokens (    0.78 ms per token,  1282.58 tokens per second)\n",
            "llama_perf_context_print:        eval time =      17.59 ms /     1 runs   (   17.59 ms per token,    56.86 tokens per second)\n",
            "llama_perf_context_print:       total time =      64.60 ms /    57 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 53 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      43.96 ms /    53 tokens (    0.83 ms per token,  1205.67 tokens per second)\n",
            "llama_perf_context_print:        eval time =      15.65 ms /     1 runs   (   15.65 ms per token,    63.91 tokens per second)\n",
            "llama_perf_context_print:       total time =      63.25 ms /    54 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 56 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      45.83 ms /    56 tokens (    0.82 ms per token,  1221.83 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.58 ms /     1 runs   (   16.58 ms per token,    60.31 tokens per second)\n",
            "llama_perf_context_print:       total time =      65.40 ms /    57 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 79 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      56.62 ms /    79 tokens (    0.72 ms per token,  1395.24 tokens per second)\n",
            "llama_perf_context_print:        eval time =      16.31 ms /     1 runs   (   16.31 ms per token,    61.31 tokens per second)\n",
            "llama_perf_context_print:       total time =      76.92 ms /    80 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 67 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      54.01 ms /    67 tokens (    0.81 ms per token,  1240.53 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      56.93 ms /    68 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 52 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      43.03 ms /    52 tokens (    0.83 ms per token,  1208.46 tokens per second)\n",
            "llama_perf_context_print:        eval time =      15.10 ms /     1 runs   (   15.10 ms per token,    66.22 tokens per second)\n",
            "llama_perf_context_print:       total time =      60.81 ms /    53 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 56 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      42.55 ms /    56 tokens (    0.76 ms per token,  1316.01 tokens per second)\n",
            "llama_perf_context_print:        eval time =      14.39 ms /     1 runs   (   14.39 ms per token,    69.48 tokens per second)\n",
            "llama_perf_context_print:       total time =      60.01 ms /    57 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 84 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      57.66 ms /    84 tokens (    0.69 ms per token,  1456.89 tokens per second)\n",
            "llama_perf_context_print:        eval time =      14.96 ms /     1 runs   (   14.96 ms per token,    66.84 tokens per second)\n",
            "llama_perf_context_print:       total time =      76.48 ms /    85 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 53 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      38.60 ms /    53 tokens (    0.73 ms per token,  1372.99 tokens per second)\n",
            "llama_perf_context_print:        eval time =      15.05 ms /     1 runs   (   15.05 ms per token,    66.45 tokens per second)\n",
            "llama_perf_context_print:       total time =      56.94 ms /    54 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 62 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      39.77 ms /    62 tokens (    0.64 ms per token,  1559.16 tokens per second)\n",
            "llama_perf_context_print:        eval time =      15.11 ms /     1 runs   (   15.11 ms per token,    66.16 tokens per second)\n",
            "llama_perf_context_print:       total time =      58.39 ms /    63 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 47 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      32.41 ms /    47 tokens (    0.69 ms per token,  1450.26 tokens per second)\n",
            "llama_perf_context_print:        eval time =      14.72 ms /     1 runs   (   14.72 ms per token,    67.95 tokens per second)\n",
            "llama_perf_context_print:       total time =      49.87 ms /    48 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 58 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      38.22 ms /    58 tokens (    0.66 ms per token,  1517.49 tokens per second)\n",
            "llama_perf_context_print:        eval time =      14.87 ms /     1 runs   (   14.87 ms per token,    67.24 tokens per second)\n",
            "llama_perf_context_print:       total time =      56.23 ms /    59 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 88 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      52.74 ms /    88 tokens (    0.60 ms per token,  1668.53 tokens per second)\n",
            "llama_perf_context_print:        eval time =      15.89 ms /     1 runs   (   15.89 ms per token,    62.94 tokens per second)\n",
            "llama_perf_context_print:       total time =      72.74 ms /    89 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 143 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      96.36 ms /   143 tokens (    0.67 ms per token,  1484.08 tokens per second)\n",
            "llama_perf_context_print:        eval time =      15.99 ms /     1 runs   (   15.99 ms per token,    62.55 tokens per second)\n",
            "llama_perf_context_print:       total time =     117.84 ms /   144 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 24 prefix-match hit, remaining 71 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      43.67 ms /    71 tokens (    0.62 ms per token,  1625.90 tokens per second)\n",
            "llama_perf_context_print:        eval time =      15.35 ms /     1 runs   (   15.35 ms per token,    65.14 tokens per second)\n",
            "llama_perf_context_print:       total time =      62.52 ms /    72 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4c6ccdb9c9c049ba8fd334b27d2500ab"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "main/train-00000-of-00001.parquet:   0%|          | 0.00/2.31M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5ebdf76863424894a1f31c2cec96e82e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "main/test-00000-of-00001.parquet:   0%|          | 0.00/419k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "94f03e45d37f4882b5c666edd1432117"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/7473 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "76c4e489e78e4b93ae6a59af33848b88"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/1319 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "07c5b775beb0455ebc0da157732682a5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =     113.66 ms /   102 tokens (    1.11 ms per token,   897.44 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2213.06 ms /   127 runs   (   17.43 ms per token,    57.39 tokens per second)\n",
            "llama_perf_context_print:       total time =    2416.24 ms /   229 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 39 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      60.94 ms /    39 tokens (    1.56 ms per token,   639.93 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1831.31 ms /   108 runs   (   16.96 ms per token,    58.97 tokens per second)\n",
            "llama_perf_context_print:       total time =    1956.86 ms /   147 tokens\n",
            "llama_perf_context_print:    graphs reused =        103\n",
            "Llama.generate: 21 prefix-match hit, remaining 70 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      87.44 ms /    70 tokens (    1.25 ms per token,   800.59 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2182.12 ms /   127 runs   (   17.18 ms per token,    58.20 tokens per second)\n",
            "llama_perf_context_print:       total time =    2361.85 ms /   197 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 46 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      63.95 ms /    46 tokens (    1.39 ms per token,   719.32 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2196.91 ms /   127 runs   (   17.30 ms per token,    57.81 tokens per second)\n",
            "llama_perf_context_print:       total time =    2339.52 ms /   173 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 147 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =     168.34 ms /   147 tokens (    1.15 ms per token,   873.22 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2229.20 ms /   127 runs   (   17.55 ms per token,    56.97 tokens per second)\n",
            "llama_perf_context_print:       total time =    2505.68 ms /   274 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 67 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      86.94 ms /    67 tokens (    1.30 ms per token,   770.63 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2206.87 ms /   127 runs   (   17.38 ms per token,    57.55 tokens per second)\n",
            "llama_perf_context_print:       total time =    2373.86 ms /   194 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 56 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      75.40 ms /    56 tokens (    1.35 ms per token,   742.74 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2206.31 ms /   127 runs   (   17.37 ms per token,    57.56 tokens per second)\n",
            "llama_perf_context_print:       total time =    2361.57 ms /   183 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 81 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      98.17 ms /    81 tokens (    1.21 ms per token,   825.12 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2243.50 ms /   127 runs   (   17.67 ms per token,    56.61 tokens per second)\n",
            "llama_perf_context_print:       total time =    2425.02 ms /   208 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 120 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =     125.16 ms /   120 tokens (    1.04 ms per token,   958.81 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2282.80 ms /   127 runs   (   17.97 ms per token,    55.63 tokens per second)\n",
            "llama_perf_context_print:       total time =    2486.39 ms /   247 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 72 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      87.85 ms /    72 tokens (    1.22 ms per token,   819.54 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2261.07 ms /   127 runs   (   17.80 ms per token,    56.17 tokens per second)\n",
            "llama_perf_context_print:       total time =    2449.05 ms /   199 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 72 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      87.80 ms /    72 tokens (    1.22 ms per token,   820.01 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2300.64 ms /   127 runs   (   18.12 ms per token,    55.20 tokens per second)\n",
            "llama_perf_context_print:       total time =    2468.19 ms /   199 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 78 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      89.98 ms /    78 tokens (    1.15 ms per token,   866.82 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2248.68 ms /   127 runs   (   17.71 ms per token,    56.48 tokens per second)\n",
            "llama_perf_context_print:       total time =    2419.80 ms /   205 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 79 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      89.19 ms /    79 tokens (    1.13 ms per token,   885.76 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2258.84 ms /   127 runs   (   17.79 ms per token,    56.22 tokens per second)\n",
            "llama_perf_context_print:       total time =    2428.48 ms /   206 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 76 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      88.73 ms /    76 tokens (    1.17 ms per token,   856.52 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2246.37 ms /   127 runs   (   17.69 ms per token,    56.54 tokens per second)\n",
            "llama_perf_context_print:       total time =    2414.82 ms /   203 tokens\n",
            "llama_perf_context_print:    graphs reused =        123\n",
            "Llama.generate: 21 prefix-match hit, remaining 68 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      87.38 ms /    68 tokens (    1.28 ms per token,   778.25 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2219.89 ms /   127 runs   (   17.48 ms per token,    57.21 tokens per second)\n",
            "llama_perf_context_print:       total time =    2404.54 ms /   195 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 115 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =     127.01 ms /   115 tokens (    1.10 ms per token,   905.41 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2251.49 ms /   127 runs   (   17.73 ms per token,    56.41 tokens per second)\n",
            "llama_perf_context_print:       total time =    2459.41 ms /   242 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 64 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      77.97 ms /    64 tokens (    1.22 ms per token,   820.87 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2223.08 ms /   127 runs   (   17.50 ms per token,    57.13 tokens per second)\n",
            "llama_perf_context_print:       total time =    2387.85 ms /   191 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 68 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      87.33 ms /    68 tokens (    1.28 ms per token,   778.63 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2210.82 ms /   127 runs   (   17.41 ms per token,    57.44 tokens per second)\n",
            "llama_perf_context_print:       total time =    2379.62 ms /   195 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 39 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      59.21 ms /    39 tokens (    1.52 ms per token,   658.65 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2232.13 ms /   127 runs   (   17.58 ms per token,    56.90 tokens per second)\n",
            "llama_perf_context_print:       total time =    2372.29 ms /   166 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 76 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      89.78 ms /    76 tokens (    1.18 ms per token,   846.47 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2226.85 ms /   127 runs   (   17.53 ms per token,    57.03 tokens per second)\n",
            "llama_perf_context_print:       total time =    2417.57 ms /   203 tokens\n",
            "llama_perf_context_print:    graphs reused =        123\n",
            "Llama.generate: 21 prefix-match hit, remaining 80 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      89.38 ms /    80 tokens (    1.12 ms per token,   895.08 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2254.79 ms /   127 runs   (   17.75 ms per token,    56.32 tokens per second)\n",
            "llama_perf_context_print:       total time =    2425.91 ms /   207 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 62 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      76.75 ms /    62 tokens (    1.24 ms per token,   807.85 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2259.72 ms /   127 runs   (   17.79 ms per token,    56.20 tokens per second)\n",
            "llama_perf_context_print:       total time =    2418.08 ms /   189 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 73 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      88.03 ms /    73 tokens (    1.21 ms per token,   829.23 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2244.38 ms /   127 runs   (   17.67 ms per token,    56.59 tokens per second)\n",
            "llama_perf_context_print:       total time =    2415.74 ms /   200 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 54 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      75.24 ms /    54 tokens (    1.39 ms per token,   717.67 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2239.25 ms /   127 runs   (   17.63 ms per token,    56.72 tokens per second)\n",
            "llama_perf_context_print:       total time =    2394.04 ms /   181 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 52 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      75.35 ms /    52 tokens (    1.45 ms per token,   690.08 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2210.30 ms /   127 runs   (   17.40 ms per token,    57.46 tokens per second)\n",
            "llama_perf_context_print:       total time =    2379.53 ms /   179 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 81 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      97.65 ms /    81 tokens (    1.21 ms per token,   829.49 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2258.08 ms /   127 runs   (   17.78 ms per token,    56.24 tokens per second)\n",
            "llama_perf_context_print:       total time =    2432.92 ms /   208 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 85 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      98.98 ms /    85 tokens (    1.16 ms per token,   858.79 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2260.84 ms /   127 runs   (   17.80 ms per token,    56.17 tokens per second)\n",
            "llama_perf_context_print:       total time =    2443.30 ms /   212 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 76 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      88.72 ms /    76 tokens (    1.17 ms per token,   856.68 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2219.91 ms /   127 runs   (   17.48 ms per token,    57.21 tokens per second)\n",
            "llama_perf_context_print:       total time =    2399.55 ms /   203 tokens\n",
            "llama_perf_context_print:    graphs reused =        123\n",
            "Llama.generate: 21 prefix-match hit, remaining 60 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      76.44 ms /    60 tokens (    1.27 ms per token,   784.92 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2239.43 ms /   127 runs   (   17.63 ms per token,    56.71 tokens per second)\n",
            "llama_perf_context_print:       total time =    2396.54 ms /   187 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 85 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      99.92 ms /    85 tokens (    1.18 ms per token,   850.67 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2231.90 ms /   127 runs   (   17.57 ms per token,    56.90 tokens per second)\n",
            "llama_perf_context_print:       total time =    2419.06 ms /   212 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 49 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      74.14 ms /    49 tokens (    1.51 ms per token,   660.92 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2212.38 ms /   127 runs   (   17.42 ms per token,    57.40 tokens per second)\n",
            "llama_perf_context_print:       total time =    2364.49 ms /   176 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 73 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      88.05 ms /    73 tokens (    1.21 ms per token,   829.09 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2232.97 ms /   127 runs   (   17.58 ms per token,    56.88 tokens per second)\n",
            "llama_perf_context_print:       total time =    2400.28 ms /   200 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 49 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      74.23 ms /    49 tokens (    1.51 ms per token,   660.07 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2212.84 ms /   127 runs   (   17.42 ms per token,    57.39 tokens per second)\n",
            "llama_perf_context_print:       total time =    2383.39 ms /   176 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 44 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      63.82 ms /    44 tokens (    1.45 ms per token,   689.40 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2242.15 ms /   127 runs   (   17.65 ms per token,    56.64 tokens per second)\n",
            "llama_perf_context_print:       total time =    2386.39 ms /   171 tokens\n",
            "llama_perf_context_print:    graphs reused =        123\n",
            "Llama.generate: 21 prefix-match hit, remaining 64 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      77.94 ms /    64 tokens (    1.22 ms per token,   821.13 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2229.37 ms /   127 runs   (   17.55 ms per token,    56.97 tokens per second)\n",
            "llama_perf_context_print:       total time =    2393.15 ms /   191 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 61 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      76.63 ms /    61 tokens (    1.26 ms per token,   796.06 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2239.14 ms /   127 runs   (   17.63 ms per token,    56.72 tokens per second)\n",
            "llama_perf_context_print:       total time =    2396.35 ms /   188 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 56 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      75.36 ms /    56 tokens (    1.35 ms per token,   743.14 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2234.94 ms /   127 runs   (   17.60 ms per token,    56.82 tokens per second)\n",
            "llama_perf_context_print:       total time =    2390.05 ms /   183 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 74 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      88.21 ms /    74 tokens (    1.19 ms per token,   838.89 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2225.94 ms /   127 runs   (   17.53 ms per token,    57.05 tokens per second)\n",
            "llama_perf_context_print:       total time =    2413.45 ms /   201 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 22 prefix-match hit, remaining 51 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      74.68 ms /    51 tokens (    1.46 ms per token,   682.89 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2221.50 ms /   127 runs   (   17.49 ms per token,    57.17 tokens per second)\n",
            "llama_perf_context_print:       total time =    2375.82 ms /   178 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 85 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      98.95 ms /    85 tokens (    1.16 ms per token,   859.03 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2249.77 ms /   127 runs   (   17.71 ms per token,    56.45 tokens per second)\n",
            "llama_perf_context_print:       total time =    2429.08 ms /   212 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 60 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      76.35 ms /    60 tokens (    1.27 ms per token,   785.86 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2221.61 ms /   127 runs   (   17.49 ms per token,    57.17 tokens per second)\n",
            "llama_perf_context_print:       total time =    2378.26 ms /   187 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 154 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =     169.85 ms /   154 tokens (    1.10 ms per token,   906.69 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2266.49 ms /   127 runs   (   17.85 ms per token,    56.03 tokens per second)\n",
            "llama_perf_context_print:       total time =    2519.69 ms /   281 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 97 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =     111.45 ms /    97 tokens (    1.15 ms per token,   870.34 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2224.39 ms /   127 runs   (   17.51 ms per token,    57.09 tokens per second)\n",
            "llama_perf_context_print:       total time =    2436.72 ms /   224 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 78 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      89.01 ms /    78 tokens (    1.14 ms per token,   876.34 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2224.15 ms /   127 runs   (   17.51 ms per token,    57.10 tokens per second)\n",
            "llama_perf_context_print:       total time =    2392.55 ms /   205 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 98 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =     111.53 ms /    98 tokens (    1.14 ms per token,   878.68 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2267.01 ms /   127 runs   (   17.85 ms per token,    56.02 tokens per second)\n",
            "llama_perf_context_print:       total time =    2460.13 ms /   225 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 112 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =     117.04 ms /   112 tokens (    1.04 ms per token,   956.96 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2259.25 ms /   127 runs   (   17.79 ms per token,    56.21 tokens per second)\n",
            "llama_perf_context_print:       total time =    2457.82 ms /   239 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 109 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =     115.84 ms /   109 tokens (    1.06 ms per token,   940.97 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2252.16 ms /   127 runs   (   17.73 ms per token,    56.39 tokens per second)\n",
            "llama_perf_context_print:       total time =    2449.99 ms /   236 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 65 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      86.61 ms /    65 tokens (    1.33 ms per token,   750.50 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2223.75 ms /   127 runs   (   17.51 ms per token,    57.11 tokens per second)\n",
            "llama_perf_context_print:       total time =    2411.55 ms /   192 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 46 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      63.96 ms /    46 tokens (    1.39 ms per token,   719.24 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1563.88 ms /    89 runs   (   17.57 ms per token,    56.91 tokens per second)\n",
            "llama_perf_context_print:       total time =    1681.31 ms /   135 tokens\n",
            "llama_perf_context_print:    graphs reused =         86\n",
            "Llama.generate: 21 prefix-match hit, remaining 55 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      75.44 ms /    55 tokens (    1.37 ms per token,   729.05 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2226.93 ms /   127 runs   (   17.53 ms per token,    57.03 tokens per second)\n",
            "llama_perf_context_print:       total time =    2402.95 ms /   182 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 47 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      64.19 ms /    47 tokens (    1.37 ms per token,   732.26 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2233.63 ms /   127 runs   (   17.59 ms per token,    56.86 tokens per second)\n",
            "llama_perf_context_print:       total time =    2376.77 ms /   174 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 61 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      77.41 ms /    61 tokens (    1.27 ms per token,   788.06 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2254.87 ms /   127 runs   (   17.75 ms per token,    56.32 tokens per second)\n",
            "llama_perf_context_print:       total time =    2411.16 ms /   188 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 79 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      89.17 ms /    79 tokens (    1.13 ms per token,   885.96 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2224.08 ms /   127 runs   (   17.51 ms per token,    57.10 tokens per second)\n",
            "llama_perf_context_print:       total time =    2393.76 ms /   206 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 137 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =     156.65 ms /   137 tokens (    1.14 ms per token,   874.58 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2254.65 ms /   127 runs   (   17.75 ms per token,    56.33 tokens per second)\n",
            "llama_perf_context_print:       total time =    2491.91 ms /   264 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 101 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =     112.01 ms /   101 tokens (    1.11 ms per token,   901.67 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2241.48 ms /   127 runs   (   17.65 ms per token,    56.66 tokens per second)\n",
            "llama_perf_context_print:       total time =    2453.78 ms /   228 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 60 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      76.92 ms /    60 tokens (    1.28 ms per token,   779.98 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2221.12 ms /   127 runs   (   17.49 ms per token,    57.18 tokens per second)\n",
            "llama_perf_context_print:       total time =    2382.30 ms /   187 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 57 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      75.96 ms /    57 tokens (    1.33 ms per token,   750.41 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2240.12 ms /   127 runs   (   17.64 ms per token,    56.69 tokens per second)\n",
            "llama_perf_context_print:       total time =    2394.94 ms /   184 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 92 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =     100.40 ms /    92 tokens (    1.09 ms per token,   916.33 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2246.26 ms /   127 runs   (   17.69 ms per token,    56.54 tokens per second)\n",
            "llama_perf_context_print:       total time =    2425.73 ms /   219 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 99 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =     111.68 ms /    99 tokens (    1.13 ms per token,   886.47 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2253.06 ms /   127 runs   (   17.74 ms per token,    56.37 tokens per second)\n",
            "llama_perf_context_print:       total time =    2443.03 ms /   226 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 44 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      63.58 ms /    44 tokens (    1.44 ms per token,   692.06 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2228.70 ms /   127 runs   (   17.55 ms per token,    56.98 tokens per second)\n",
            "llama_perf_context_print:       total time =    2389.35 ms /   171 tokens\n",
            "llama_perf_context_print:    graphs reused =        123\n",
            "Llama.generate: 22 prefix-match hit, remaining 49 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      74.27 ms /    49 tokens (    1.52 ms per token,   659.74 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2228.34 ms /   127 runs   (   17.55 ms per token,    56.99 tokens per second)\n",
            "llama_perf_context_print:       total time =    2379.88 ms /   176 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 73 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      88.81 ms /    73 tokens (    1.22 ms per token,   821.94 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2232.89 ms /   127 runs   (   17.58 ms per token,    56.88 tokens per second)\n",
            "llama_perf_context_print:       total time =    2407.46 ms /   200 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 81 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      98.06 ms /    81 tokens (    1.21 ms per token,   826.06 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2235.88 ms /   127 runs   (   17.61 ms per token,    56.80 tokens per second)\n",
            "llama_perf_context_print:       total time =    2413.92 ms /   208 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 78 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      89.19 ms /    78 tokens (    1.14 ms per token,   874.52 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2235.79 ms /   127 runs   (   17.60 ms per token,    56.80 tokens per second)\n",
            "llama_perf_context_print:       total time =    2405.84 ms /   205 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 107 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =     113.71 ms /   107 tokens (    1.06 ms per token,   941.01 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2259.43 ms /   127 runs   (   17.79 ms per token,    56.21 tokens per second)\n",
            "llama_perf_context_print:       total time =    2464.72 ms /   234 tokens\n",
            "llama_perf_context_print:    graphs reused =        123\n",
            "Llama.generate: 21 prefix-match hit, remaining 57 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      75.96 ms /    57 tokens (    1.33 ms per token,   750.36 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2248.77 ms /   127 runs   (   17.71 ms per token,    56.48 tokens per second)\n",
            "llama_perf_context_print:       total time =    2402.75 ms /   184 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 67 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      87.58 ms /    67 tokens (    1.31 ms per token,   765.03 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2222.07 ms /   127 runs   (   17.50 ms per token,    57.15 tokens per second)\n",
            "llama_perf_context_print:       total time =    2398.47 ms /   194 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 71 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      87.58 ms /    71 tokens (    1.23 ms per token,   810.71 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2236.59 ms /   127 runs   (   17.61 ms per token,    56.78 tokens per second)\n",
            "llama_perf_context_print:       total time =    2402.97 ms /   198 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 47 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      64.18 ms /    47 tokens (    1.37 ms per token,   732.28 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2211.37 ms /   127 runs   (   17.41 ms per token,    57.43 tokens per second)\n",
            "llama_perf_context_print:       total time =    2354.25 ms /   174 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 63 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      76.97 ms /    63 tokens (    1.22 ms per token,   818.46 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2240.99 ms /   127 runs   (   17.65 ms per token,    56.67 tokens per second)\n",
            "llama_perf_context_print:       total time =    2401.36 ms /   190 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 63 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      76.93 ms /    63 tokens (    1.22 ms per token,   818.93 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2240.00 ms /   127 runs   (   17.64 ms per token,    56.70 tokens per second)\n",
            "llama_perf_context_print:       total time =    2396.96 ms /   190 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 56 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      76.42 ms /    56 tokens (    1.36 ms per token,   732.76 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2212.92 ms /   127 runs   (   17.42 ms per token,    57.39 tokens per second)\n",
            "llama_perf_context_print:       total time =    2384.24 ms /   183 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 66 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      86.71 ms /    66 tokens (    1.31 ms per token,   761.12 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2221.22 ms /   127 runs   (   17.49 ms per token,    57.18 tokens per second)\n",
            "llama_perf_context_print:       total time =    2386.80 ms /   193 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 57 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      75.90 ms /    57 tokens (    1.33 ms per token,   750.97 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2235.49 ms /   127 runs   (   17.60 ms per token,    56.81 tokens per second)\n",
            "llama_perf_context_print:       total time =    2399.80 ms /   184 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 142 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =     167.17 ms /   142 tokens (    1.18 ms per token,   849.44 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2275.03 ms /   127 runs   (   17.91 ms per token,    55.82 tokens per second)\n",
            "llama_perf_context_print:       total time =    2532.89 ms /   269 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 81 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      97.83 ms /    81 tokens (    1.21 ms per token,   828.00 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2257.47 ms /   127 runs   (   17.78 ms per token,    56.26 tokens per second)\n",
            "llama_perf_context_print:       total time =    2435.10 ms /   208 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 102 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =     113.28 ms /   102 tokens (    1.11 ms per token,   900.42 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2259.48 ms /   127 runs   (   17.79 ms per token,    56.21 tokens per second)\n",
            "llama_perf_context_print:       total time =    2463.02 ms /   229 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 61 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      76.58 ms /    61 tokens (    1.26 ms per token,   796.55 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2245.56 ms /   127 runs   (   17.68 ms per token,    56.56 tokens per second)\n",
            "llama_perf_context_print:       total time =    2400.84 ms /   188 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 51 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      74.67 ms /    51 tokens (    1.46 ms per token,   682.98 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2205.35 ms /   127 runs   (   17.36 ms per token,    57.59 tokens per second)\n",
            "llama_perf_context_print:       total time =    2358.14 ms /   178 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 59 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      76.38 ms /    59 tokens (    1.29 ms per token,   772.46 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2245.90 ms /   127 runs   (   17.68 ms per token,    56.55 tokens per second)\n",
            "llama_perf_context_print:       total time =    2401.66 ms /   186 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 46 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      64.05 ms /    46 tokens (    1.39 ms per token,   718.19 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2215.24 ms /   127 runs   (   17.44 ms per token,    57.33 tokens per second)\n",
            "llama_perf_context_print:       total time =    2356.87 ms /   173 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 76 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      88.60 ms /    76 tokens (    1.17 ms per token,   857.75 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2237.10 ms /   127 runs   (   17.61 ms per token,    56.77 tokens per second)\n",
            "llama_perf_context_print:       total time =    2421.64 ms /   203 tokens\n",
            "llama_perf_context_print:    graphs reused =        123\n",
            "Llama.generate: 21 prefix-match hit, remaining 44 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      63.56 ms /    44 tokens (    1.44 ms per token,   692.28 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2235.62 ms /   127 runs   (   17.60 ms per token,    56.81 tokens per second)\n",
            "llama_perf_context_print:       total time =    2377.40 ms /   171 tokens\n",
            "llama_perf_context_print:    graphs reused =        123\n",
            "Llama.generate: 21 prefix-match hit, remaining 44 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      63.55 ms /    44 tokens (    1.44 ms per token,   692.35 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2001.86 ms /   114 runs   (   17.56 ms per token,    56.95 tokens per second)\n",
            "llama_perf_context_print:       total time =    2135.17 ms /   158 tokens\n",
            "llama_perf_context_print:    graphs reused =        110\n",
            "Llama.generate: 21 prefix-match hit, remaining 32 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      54.68 ms /    32 tokens (    1.71 ms per token,   585.23 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2211.21 ms /   127 runs   (   17.41 ms per token,    57.43 tokens per second)\n",
            "llama_perf_context_print:       total time =    2344.73 ms /   159 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 105 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =     113.18 ms /   105 tokens (    1.08 ms per token,   927.76 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2263.59 ms /   127 runs   (   17.82 ms per token,    56.11 tokens per second)\n",
            "llama_perf_context_print:       total time =    2456.12 ms /   232 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 97 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =     111.45 ms /    97 tokens (    1.15 ms per token,   870.31 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2264.99 ms /   127 runs   (   17.83 ms per token,    56.07 tokens per second)\n",
            "llama_perf_context_print:       total time =    2469.23 ms /   224 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 92 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =     100.36 ms /    92 tokens (    1.09 ms per token,   916.66 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2257.37 ms /   127 runs   (   17.77 ms per token,    56.26 tokens per second)\n",
            "llama_perf_context_print:       total time =    2437.74 ms /   219 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 50 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      75.11 ms /    50 tokens (    1.50 ms per token,   665.71 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2226.44 ms /   127 runs   (   17.53 ms per token,    57.04 tokens per second)\n",
            "llama_perf_context_print:       total time =    2388.77 ms /   177 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 66 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      86.97 ms /    66 tokens (    1.32 ms per token,   758.90 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2242.20 ms /   127 runs   (   17.66 ms per token,    56.64 tokens per second)\n",
            "llama_perf_context_print:       total time =    2407.78 ms /   193 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 110 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =     116.29 ms /   110 tokens (    1.06 ms per token,   945.91 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2244.62 ms /   127 runs   (   17.67 ms per token,    56.58 tokens per second)\n",
            "llama_perf_context_print:       total time =    2442.08 ms /   237 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 52 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      74.76 ms /    52 tokens (    1.44 ms per token,   695.54 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2213.43 ms /   127 runs   (   17.43 ms per token,    57.38 tokens per second)\n",
            "llama_perf_context_print:       total time =    2376.56 ms /   179 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 64 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      77.78 ms /    64 tokens (    1.22 ms per token,   822.87 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2219.32 ms /   127 runs   (   17.47 ms per token,    57.22 tokens per second)\n",
            "llama_perf_context_print:       total time =    2374.92 ms /   191 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 109 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =     117.45 ms /   109 tokens (    1.08 ms per token,   928.04 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2248.64 ms /   127 runs   (   17.71 ms per token,    56.48 tokens per second)\n",
            "llama_perf_context_print:       total time =    2446.50 ms /   236 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 64 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      77.11 ms /    64 tokens (    1.20 ms per token,   829.99 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2217.30 ms /   127 runs   (   17.46 ms per token,    57.28 tokens per second)\n",
            "llama_perf_context_print:       total time =    2372.80 ms /   191 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 53 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      75.01 ms /    53 tokens (    1.42 ms per token,   706.55 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2228.99 ms /   127 runs   (   17.55 ms per token,    56.98 tokens per second)\n",
            "llama_perf_context_print:       total time =    2382.68 ms /   180 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n",
            "Llama.generate: 21 prefix-match hit, remaining 43 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      62.98 ms /    43 tokens (    1.46 ms per token,   682.75 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2241.64 ms /   127 runs   (   17.65 ms per token,    56.66 tokens per second)\n",
            "llama_perf_context_print:       total time =    2398.01 ms /   170 tokens\n",
            "llama_perf_context_print:    graphs reused =        123\n",
            "Llama.generate: 21 prefix-match hit, remaining 76 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      88.68 ms /    76 tokens (    1.17 ms per token,   857.00 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2251.58 ms /   127 runs   (   17.73 ms per token,    56.40 tokens per second)\n",
            "llama_perf_context_print:       total time =    2419.05 ms /   203 tokens\n",
            "llama_perf_context_print:    graphs reused =        123\n",
            "Llama.generate: 21 prefix-match hit, remaining 107 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =     114.39 ms /   107 tokens (    1.07 ms per token,   935.36 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2282.05 ms /   127 runs   (   17.97 ms per token,    55.65 tokens per second)\n",
            "llama_perf_context_print:       total time =    2481.73 ms /   234 tokens\n",
            "llama_perf_context_print:    graphs reused =        123\n",
            "Llama.generate: 21 prefix-match hit, remaining 98 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =     111.80 ms /    98 tokens (    1.14 ms per token,   876.59 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2239.43 ms /   127 runs   (   17.63 ms per token,    56.71 tokens per second)\n",
            "llama_perf_context_print:       total time =    2429.16 ms /   225 tokens\n",
            "llama_perf_context_print:    graphs reused =        122\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MMLU: {\n",
            "  \"n_total\": 200,\n",
            "  \"n_eval\": 168,\n",
            "  \"coverage\": 0.84,\n",
            "  \"accuracy\": 0.27380952380952384,\n",
            "  \"precision_macro\": 0.26916866028708136,\n",
            "  \"recall_macro\": 0.17577519379844958,\n",
            "  \"f1_macro\": 0.21137566137566138,\n",
            "  \"confusion_matrix\": [\n",
            "    [\n",
            "      34,\n",
            "      31,\n",
            "      3,\n",
            "      7\n",
            "    ],\n",
            "    [\n",
            "      7,\n",
            "      3,\n",
            "      28,\n",
            "      5\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      4,\n",
            "      9,\n",
            "      34\n",
            "    ],\n",
            "    [\n",
            "      0,\n",
            "      0,\n",
            "      0,\n",
            "      0\n",
            "    ]\n",
            "  ],\n",
            "  \"latency\": {\n",
            "    \"p50\": 0.1750345230102539,\n",
            "    \"p90\": 0.24130010604858398,\n",
            "    \"mean\": 0.18871362686157225\n",
            "  },\n",
            "  \"throughput\": {\n",
            "    \"tps_mean\": 11.145415350611902\n",
            "  },\n",
            "  \"per_subject_accuracy\": {\n",
            "    \"college_computer_science\": 0.5555555555555556,\n",
            "    \"abstract_algebra\": 0.5,\n",
            "    \"astronomy\": 0.4666666666666667,\n",
            "    \"business_ethics\": 0.4444444444444444,\n",
            "    \"c …\n",
            "GSM8K: {\n",
            "  \"n_total\": 100,\n",
            "  \"accuracy_em\": 0.16,\n",
            "  \"format_error_rate\": 0.0,\n",
            "  \"latency\": {\n",
            "    \"p50\": 7.14889931678772,\n",
            "    \"p90\": 7.721417427062988,\n",
            "    \"mean\": 7.196120216846466\n",
            "  },\n",
            "  \"throughput\": {\n",
            "    \"tps_mean\": 17.746592404589013\n",
            "  }\n",
            "} …\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Persist results (for compare across models later)"
      ],
      "metadata": {
        "id": "nuateHyF14CA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- persist run outputs -----------------------------------------------------\n",
        "import uuid, pathlib, time, json\n",
        "from dataclasses import asdict\n",
        "\n",
        "# prefer /content on Colab; otherwise fall back to a local folder\n",
        "run_id = uuid.uuid4().hex[:8]\n",
        "out_dir = pathlib.Path(\"/content/llm-bench-runs\")\n",
        "if not out_dir.exists():\n",
        "    out_dir = pathlib.Path(\"./llm-bench-runs\")\n",
        "out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "summary = {\n",
        "    \"run_id\": run_id,\n",
        "    \"model\": cfg.model_name,\n",
        "    \"quant\": cfg.quant,\n",
        "    \"hardware\": {\"gpu\": \"T4\", \"gpu_ram_gb\": 15, \"sys_ram_gb\": 12.7},  # tweak if needed\n",
        "    \"settings\": asdict(cfg),\n",
        "    \"mmlu\": mmlu_metrics,\n",
        "    \"gsm8k\": gsm_metrics,\n",
        "    \"timestamp\": time.time(),\n",
        "}\n",
        "\n",
        "(out_dir / f\"{run_id}_summary.json\").write_text(json.dumps(summary, indent=2))\n",
        "mmlu_df.to_json(out_dir / f\"{run_id}_mmlu_preds.jsonl\", orient=\"records\", lines=True)\n",
        "gsm_df.to_json(out_dir / f\"{run_id}_gsm_preds.jsonl\", orient=\"records\", lines=True)\n",
        "\n",
        "print(\"Saved under:\", out_dir.resolve())\n",
        "for p in sorted(out_dir.glob(f\"{run_id}_*\")):\n",
        "    print(\" -\", p.name)\n",
        "# -----------------------------------------------------------------------------\n"
      ],
      "metadata": {
        "id": "lsK6ZH8S14Vl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1813d5f3-1950-4851-e370-3294ad3e0b72"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved under: /content/llm-bench-runs\n",
            " - dfc8d221_gsm_preds.jsonl\n",
            " - dfc8d221_mmlu_preds.jsonl\n",
            " - dfc8d221_summary.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ARC-C + HellaSwag evaluators"
      ],
      "metadata": {
        "id": "DxZIME7tq3ju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ARC-Challenge & HellaSwag evaluators (complete)\n",
        "\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import List, Tuple, Optional, Dict, Any\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time, re, json, math\n",
        "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
        "\n",
        "# ---------- Shared helpers ----------\n",
        "\n",
        "LETTER_RE = re.compile(r\"\\b([A-Z])\\b\")\n",
        "\n",
        "def parse_letter_from_set(text: str, allowed: List[str]) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Robustly parse a single-letter answer from {allowed}.\n",
        "    Tries strict regex first, then a looser scan.\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return None\n",
        "    m = LETTER_RE.search(text.upper())\n",
        "    if m:\n",
        "        c = m.group(1)\n",
        "        if c in allowed:\n",
        "            return c\n",
        "    up = text.upper()\n",
        "    for c in allowed:\n",
        "        if re.search(rf\"\\b{c}\\b\", up):\n",
        "            return c\n",
        "    return None\n",
        "\n",
        "def mcq_prompt_generic(question: str, choices: List[str], letters: List[str]) -> str:\n",
        "    lines = [question.strip(), \"\"]\n",
        "    for L, opt in zip(letters, choices):\n",
        "        lines.append(f\"{L}. {opt.strip()}\")\n",
        "    lines.append(\"\")\n",
        "    letters_slash = \"/\".join(letters)\n",
        "    lines.append(f\"Choose the single best answer. Reply with one letter only ({letters_slash}).\")\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "# ---------- ARC-Challenge ----------\n",
        "\n",
        "def arc_iter(split: str = \"validation\", limit: Optional[int] = None):\n",
        "    \"\"\"\n",
        "    Iterates ARC-Challenge (ai2_arc) with normalized A..E letters and gold = answerKey.\n",
        "    \"\"\"\n",
        "    ds = load_dataset(\"ai2_arc\", \"ARC-Challenge\", split=split)\n",
        "    for i, ex in enumerate(ds):\n",
        "        if limit is not None and i >= limit: break\n",
        "        q = ex[\"question\"]\n",
        "        labels = ex[\"choices\"][\"label\"]   # e.g. [\"A\",\"B\",\"C\",\"D\"] (sometimes \"E\")\n",
        "        texts  = ex[\"choices\"][\"text\"]\n",
        "        # Canonicalize A.. order\n",
        "        pairs = sorted(zip(labels, texts), key=lambda p: p[0])\n",
        "        letters = [L for L,_ in pairs]\n",
        "        opts    = [t for _,t in pairs]\n",
        "        gold = ex.get(\"answerKey\")\n",
        "        gold = gold.strip().upper() if isinstance(gold, str) else None\n",
        "        yield {\"id\": f\"arc-{i}\", \"q\": q, \"letters\": letters, \"choices\": opts, \"gold\": gold}\n",
        "\n",
        "def evaluate_arc(cfg: RunCfg, split: str = \"validation\", limit: Optional[int] = None):\n",
        "    rows = []\n",
        "    for ex in arc_iter(split, limit):\n",
        "        prompt = mcq_prompt_generic(ex[\"q\"], ex[\"choices\"], ex[\"letters\"])\n",
        "        out, dt, tps, top = run_llm(prompt, max_tokens=2, temperature=cfg.temperature, stop=[\"\\n\"])\n",
        "        pred = parse_letter_from_set(out, ex[\"letters\"])\n",
        "        rows.append({\n",
        "            \"id\": ex[\"id\"], \"gold\": ex[\"gold\"], \"pred\": pred,\n",
        "            \"letters\": \"\".join(ex[\"letters\"]),\n",
        "            \"latency_s\": dt, \"toks_per_s\": tps, \"raw\": out, \"top_logprobs\": top\n",
        "        })\n",
        "    df = pd.DataFrame(rows)\n",
        "\n",
        "    df[\"correct\"] = (df[\"pred\"] == df[\"gold\"])\n",
        "    acc = df[\"correct\"].mean()\n",
        "\n",
        "    # dynamic label set (handles occasional 'E')\n",
        "    in_true = set(df[\"gold\"].dropna().unique().tolist())\n",
        "    in_pred = set(df[\"pred\"].dropna().unique().tolist())\n",
        "    labels = sorted(list((in_true | in_pred) or set([\"A\",\"B\",\"C\",\"D\"])))\n",
        "\n",
        "    y_true = df[\"gold\"].fillna(\"Z\")\n",
        "    y_pred = df[\"pred\"].fillna(\"Z\")\n",
        "\n",
        "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
        "        y_true, y_pred, labels=labels, average=\"macro\", zero_division=0\n",
        "    )\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=labels).tolist()\n",
        "    fmt_err = df[\"pred\"].isna().mean()\n",
        "\n",
        "    lat = {\"p50\": df[\"latency_s\"].median(), \"p90\": df[\"latency_s\"].quantile(0.9), \"mean\": df[\"latency_s\"].mean()}\n",
        "    tps = df[\"toks_per_s\"].dropna()\n",
        "    perf = {\"tps_mean\": float(tps.mean()) if not tps.empty else None}\n",
        "\n",
        "    metrics = {\n",
        "        \"accuracy\": float(acc) if not math.isnan(acc) else None,\n",
        "        \"precision_macro\": float(prec),\n",
        "        \"recall_macro\": float(rec),\n",
        "        \"f1_macro\": float(f1),\n",
        "        \"confusion_matrix_labels\": labels,\n",
        "        \"confusion_matrix\": cm,\n",
        "        \"format_error_rate\": float(fmt_err),\n",
        "        \"latency\": lat,\n",
        "        \"throughput\": perf,\n",
        "    }\n",
        "    return df, metrics\n",
        "\n",
        "# ---------- HellaSwag ----------\n",
        "\n",
        "def hellaswag_iter(split: str = \"validation\", limit: Optional[int] = None):\n",
        "    \"\"\"\n",
        "    Iterates HellaSwag with A..D choices. Use 'validation' for labeled eval.\n",
        "    \"\"\"\n",
        "    ds = load_dataset(\"hellaswag\", split=split)  # 'validation' is labeled; 'test' is unlabeled\n",
        "    for i, ex in enumerate(ds):\n",
        "        if limit is not None and i >= limit: break\n",
        "        ctx = ex[\"ctx\"]\n",
        "        endings = ex[\"endings\"]  # list of 4\n",
        "        letters = [\"A\",\"B\",\"C\",\"D\"]\n",
        "        gold_idx = ex.get(\"label\", None)\n",
        "        gold = letters[gold_idx] if gold_idx is not None else None\n",
        "        q = ctx.strip() + \"\\n\\nWhich ending is most plausible?\"\n",
        "        yield {\"id\": f\"hs-{i}\", \"q\": q, \"choices\": endings, \"letters\": letters, \"gold\": gold}\n",
        "\n",
        "def evaluate_hellaswag(cfg: RunCfg, split: str = \"validation\", limit: Optional[int] = None):\n",
        "    rows = []\n",
        "    for ex in hellaswag_iter(split, limit):\n",
        "        prompt = mcq_prompt_generic(ex[\"q\"], ex[\"choices\"], ex[\"letters\"])\n",
        "        out, dt, tps, top = run_llm(prompt, max_tokens=2, temperature=cfg.temperature, stop=[\"\\n\"])\n",
        "        pred = parse_letter_from_set(out, ex[\"letters\"])\n",
        "        rows.append({\n",
        "            \"id\": ex[\"id\"], \"gold\": ex[\"gold\"], \"pred\": pred,\n",
        "            \"latency_s\": dt, \"toks_per_s\": tps, \"raw\": out, \"top_logprobs\": top\n",
        "        })\n",
        "    df = pd.DataFrame(rows)\n",
        "    df[\"correct\"] = (df[\"pred\"] == df[\"gold\"])\n",
        "    acc = df[\"correct\"].mean()\n",
        "\n",
        "    y_true = df[\"gold\"].fillna(\"Z\")\n",
        "    y_pred = df[\"pred\"].fillna(\"Z\")\n",
        "    labels = [\"A\",\"B\",\"C\",\"D\"]\n",
        "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
        "        y_true, y_pred, labels=labels, average=\"macro\", zero_division=0\n",
        "    )\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=labels).tolist()\n",
        "    fmt_err = df[\"pred\"].isna().mean()\n",
        "\n",
        "    lat = {\"p50\": df[\"latency_s\"].median(), \"p90\": df[\"latency_s\"].quantile(0.9), \"mean\": df[\"latency_s\"].mean()}\n",
        "    tps = df[\"toks_per_s\"].dropna()\n",
        "    perf = {\"tps_mean\": float(tps.mean()) if not tps.empty else None}\n",
        "\n",
        "    metrics = {\n",
        "        \"accuracy\": float(acc) if not math.isnan(acc) else None,\n",
        "        \"precision_macro\": float(prec),\n",
        "        \"recall_macro\": float(rec),\n",
        "        \"f1_macro\": float(f1),\n",
        "        \"confusion_matrix\": cm,\n",
        "        \"format_error_rate\": float(fmt_err),\n",
        "        \"latency\": lat,\n",
        "        \"throughput\": perf,\n",
        "    }\n",
        "    return df, metrics\n"
      ],
      "metadata": {
        "id": "fdNvwasPqy8J"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Smoke test"
      ],
      "metadata": {
        "id": "dweIcYKCq_bG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    _ = cfg  # already set by earlier MMLU/GSM8K smoke test\n",
        "except NameError:\n",
        "    cfg = RunCfg(model_name=\"phi-3.5-mini-instruct\", quant=\"Q4_K_M\", temperature=0.0, seed=1234)\n",
        "\n",
        "print(\"ARC-Challenge (validation, 100 ex)\")\n",
        "arc_df, arc_metrics = evaluate_arc(cfg, split=\"validation\", limit=100)\n",
        "print(json.dumps(arc_metrics, indent=2)[:600], \"...\\n\")\n",
        "display(arc_df.head())\n",
        "\n",
        "print(\"\\nHellaSwag (validation, 100 ex)\")\n",
        "hs_df, hs_metrics = evaluate_hellaswag(cfg, split=\"validation\", limit=100)\n",
        "print(json.dumps(hs_metrics, indent=2)[:600], \"...\\n\")\n",
        "display(hs_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "A1kaNGr8q_PV",
        "outputId": "ce2028a6-1dd6-43f4-aff9-e6d07d5ab312"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ARC-Challenge (validation, 100 ex)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =     114.20 ms /   104 tokens (    1.10 ms per token,   910.69 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =     119.40 ms /   105 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      79.10 ms /    90 tokens (    0.88 ms per token,  1137.81 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      82.27 ms /    91 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =     135.81 ms /   192 tokens (    0.71 ms per token,  1413.79 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =     141.80 ms /   193 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      57.36 ms /    84 tokens (    0.68 ms per token,  1464.41 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      60.41 ms /    85 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      61.51 ms /   105 tokens (    0.59 ms per token,  1706.93 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      65.07 ms /   106 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      44.35 ms /    69 tokens (    0.64 ms per token,  1555.77 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      46.76 ms /    70 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      46.54 ms /    78 tokens (    0.60 ms per token,  1676.05 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      48.74 ms /    79 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      52.91 ms /    85 tokens (    0.62 ms per token,  1606.65 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      55.94 ms /    86 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      62.51 ms /   109 tokens (    0.57 ms per token,  1743.83 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      66.00 ms /   110 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      44.95 ms /    71 tokens (    0.63 ms per token,  1579.36 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      47.39 ms /    72 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      70.56 ms /   120 tokens (    0.59 ms per token,  1700.70 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      74.31 ms /   121 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      69.83 ms /   122 tokens (    0.57 ms per token,  1747.18 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      73.59 ms /   123 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      96.98 ms /   143 tokens (    0.68 ms per token,  1474.58 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =     101.35 ms /   144 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      59.66 ms /    93 tokens (    0.64 ms per token,  1558.76 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      62.63 ms /    94 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      55.89 ms /    81 tokens (    0.69 ms per token,  1449.17 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      58.59 ms /    82 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      76.85 ms /   117 tokens (    0.66 ms per token,  1522.51 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      80.75 ms /   118 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      61.61 ms /   106 tokens (    0.58 ms per token,  1720.61 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      65.07 ms /   107 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      39.14 ms /    63 tokens (    0.62 ms per token,  1609.57 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      41.45 ms /    64 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      53.05 ms /    83 tokens (    0.64 ms per token,  1564.65 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      55.32 ms /    84 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      62.37 ms /   108 tokens (    0.58 ms per token,  1731.69 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      65.94 ms /   109 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      37.67 ms /    57 tokens (    0.66 ms per token,  1513.22 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      39.89 ms /    58 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      63.42 ms /   112 tokens (    0.57 ms per token,  1765.89 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      66.83 ms /   113 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      72.77 ms /   128 tokens (    0.57 ms per token,  1758.89 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      76.82 ms /   129 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      44.44 ms /    67 tokens (    0.66 ms per token,  1507.62 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      47.10 ms /    68 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      70.56 ms /   116 tokens (    0.61 ms per token,  1643.92 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      74.66 ms /   117 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      60.63 ms /   102 tokens (    0.59 ms per token,  1682.36 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      64.25 ms /   103 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      39.19 ms /    61 tokens (    0.64 ms per token,  1556.40 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      41.67 ms /    62 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      96.38 ms /   132 tokens (    0.73 ms per token,  1369.54 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =     100.70 ms /   133 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      38.36 ms /    53 tokens (    0.72 ms per token,  1381.65 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      40.51 ms /    54 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      47.53 ms /    76 tokens (    0.63 ms per token,  1599.12 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      50.15 ms /    77 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =     115.97 ms /   168 tokens (    0.69 ms per token,  1448.71 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =     121.37 ms /   169 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      61.15 ms /   100 tokens (    0.61 ms per token,  1635.43 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      64.44 ms /   101 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =     109.12 ms /   181 tokens (    0.60 ms per token,  1658.69 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =     114.45 ms /   182 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =     118.62 ms /   184 tokens (    0.64 ms per token,  1551.13 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =     125.12 ms /   185 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      61.08 ms /   104 tokens (    0.59 ms per token,  1702.63 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      64.89 ms /   105 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      30.87 ms /    48 tokens (    0.64 ms per token,  1555.06 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      32.87 ms /    49 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 1 prefix-match hit, remaining 83 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      52.70 ms /    83 tokens (    0.63 ms per token,  1575.10 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      56.08 ms /    84 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      98.80 ms /   151 tokens (    0.65 ms per token,  1528.36 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =     103.95 ms /   152 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      63.74 ms /   111 tokens (    0.57 ms per token,  1741.37 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      67.77 ms /   112 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      71.08 ms /   124 tokens (    0.57 ms per token,  1744.59 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      74.76 ms /   125 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      96.33 ms /   134 tokens (    0.72 ms per token,  1391.01 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =     100.89 ms /   135 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      64.21 ms /   111 tokens (    0.58 ms per token,  1728.60 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      67.62 ms /   112 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      61.23 ms /   102 tokens (    0.60 ms per token,  1665.88 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      64.46 ms /   103 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      96.86 ms /   133 tokens (    0.73 ms per token,  1373.17 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =     100.86 ms /   134 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      60.35 ms /   104 tokens (    0.58 ms per token,  1723.34 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      63.65 ms /   105 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      46.86 ms /    78 tokens (    0.60 ms per token,  1664.43 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      49.24 ms /    79 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      47.62 ms /    79 tokens (    0.60 ms per token,  1658.93 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      49.97 ms /    80 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      39.23 ms /    60 tokens (    0.65 ms per token,  1529.29 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      41.24 ms /    61 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      96.68 ms /   134 tokens (    0.72 ms per token,  1386.00 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =     101.17 ms /   135 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      44.49 ms /    69 tokens (    0.64 ms per token,  1550.88 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      47.02 ms /    70 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      70.31 ms /   118 tokens (    0.60 ms per token,  1678.16 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      74.07 ms /   119 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      61.33 ms /   102 tokens (    0.60 ms per token,  1663.16 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      64.30 ms /   103 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      61.54 ms /   105 tokens (    0.59 ms per token,  1706.18 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      64.63 ms /   106 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      45.46 ms /    73 tokens (    0.62 ms per token,  1605.84 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      47.78 ms /    74 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 1 prefix-match hit, remaining 63 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      39.79 ms /    63 tokens (    0.63 ms per token,  1583.39 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      42.17 ms /    64 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      67.02 ms /   110 tokens (    0.61 ms per token,  1641.23 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      70.30 ms /   111 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      40.72 ms /    51 tokens (    0.80 ms per token,  1252.36 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      42.45 ms /    52 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      46.36 ms /    73 tokens (    0.64 ms per token,  1574.53 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      48.58 ms /    74 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 1 prefix-match hit, remaining 61 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      38.79 ms /    61 tokens (    0.64 ms per token,  1572.53 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      40.91 ms /    62 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      97.01 ms /   136 tokens (    0.71 ms per token,  1401.86 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =     101.08 ms /   137 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      54.86 ms /    91 tokens (    0.60 ms per token,  1658.86 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      57.79 ms /    92 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      61.65 ms /   107 tokens (    0.58 ms per token,  1735.58 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      64.80 ms /   108 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      44.46 ms /    66 tokens (    0.67 ms per token,  1484.48 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      46.69 ms /    67 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      61.13 ms /   102 tokens (    0.60 ms per token,  1668.55 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      64.25 ms /   103 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      72.64 ms /   124 tokens (    0.59 ms per token,  1707.14 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      76.18 ms /   125 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      45.59 ms /    69 tokens (    0.66 ms per token,  1513.39 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      47.86 ms /    70 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      53.24 ms /    90 tokens (    0.59 ms per token,  1690.39 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      55.81 ms /    91 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 1 prefix-match hit, remaining 95 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      54.58 ms /    95 tokens (    0.57 ms per token,  1740.53 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      57.55 ms /    96 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      61.02 ms /   100 tokens (    0.61 ms per token,  1638.73 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      64.15 ms /   101 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      61.13 ms /   102 tokens (    0.60 ms per token,  1668.60 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      64.43 ms /   103 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      47.65 ms /    80 tokens (    0.60 ms per token,  1678.94 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      50.29 ms /    81 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      71.74 ms /   118 tokens (    0.61 ms per token,  1644.78 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      75.10 ms /   119 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      53.05 ms /    86 tokens (    0.62 ms per token,  1621.17 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      55.69 ms /    87 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      72.31 ms /   123 tokens (    0.59 ms per token,  1701.13 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      76.19 ms /   124 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      54.66 ms /    95 tokens (    0.58 ms per token,  1738.05 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      57.51 ms /    96 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      45.15 ms /    69 tokens (    0.65 ms per token,  1528.31 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      47.43 ms /    70 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      62.78 ms /   106 tokens (    0.59 ms per token,  1688.33 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      65.99 ms /   107 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      62.82 ms /   104 tokens (    0.60 ms per token,  1655.55 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      66.09 ms /   105 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      53.99 ms /    94 tokens (    0.57 ms per token,  1741.16 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      56.91 ms /    95 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      63.60 ms /   112 tokens (    0.57 ms per token,  1760.98 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      67.43 ms /   113 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      97.53 ms /   144 tokens (    0.68 ms per token,  1476.53 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =     101.95 ms /   145 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      38.42 ms /    58 tokens (    0.66 ms per token,  1509.63 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      40.49 ms /    59 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      61.30 ms /    98 tokens (    0.63 ms per token,  1598.59 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      64.48 ms /    99 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      63.00 ms /   105 tokens (    0.60 ms per token,  1666.75 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      66.21 ms /   106 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "Llama.generate: 1 prefix-match hit, remaining 127 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      71.83 ms /   127 tokens (    0.57 ms per token,  1768.14 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      75.69 ms /   128 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      38.40 ms /    64 tokens (    0.60 ms per token,  1666.71 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      40.46 ms /    65 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      54.75 ms /    94 tokens (    0.58 ms per token,  1716.86 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      57.94 ms /    95 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      47.59 ms /    77 tokens (    0.62 ms per token,  1617.92 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      49.95 ms /    78 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      99.57 ms /   152 tokens (    0.66 ms per token,  1526.63 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =     104.25 ms /   153 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      95.70 ms /   138 tokens (    0.69 ms per token,  1441.93 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      99.83 ms /   139 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      61.72 ms /   101 tokens (    0.61 ms per token,  1636.42 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      64.97 ms /   102 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      70.45 ms /   115 tokens (    0.61 ms per token,  1632.34 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      73.87 ms /   116 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      54.87 ms /    94 tokens (    0.58 ms per token,  1713.05 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      57.78 ms /    95 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      53.16 ms /    84 tokens (    0.63 ms per token,  1580.19 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      55.66 ms /    85 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      54.40 ms /    90 tokens (    0.60 ms per token,  1654.53 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      57.15 ms /    91 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      73.16 ms /   126 tokens (    0.58 ms per token,  1722.32 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      77.07 ms /   127 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      52.27 ms /    84 tokens (    0.62 ms per token,  1607.16 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      55.27 ms /    85 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      47.15 ms /    80 tokens (    0.59 ms per token,  1696.82 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      49.89 ms /    81 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      54.26 ms /    90 tokens (    0.60 ms per token,  1658.77 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      57.23 ms /    91 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n",
            "llama_perf_context_print:        load time =      58.75 ms\n",
            "llama_perf_context_print: prompt eval time =      53.62 ms /    84 tokens (    0.64 ms per token,  1566.55 tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =      56.23 ms /    85 tokens\n",
            "llama_perf_context_print:    graphs reused =          0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"accuracy\": 0.0,\n",
            "  \"precision_macro\": 0.0,\n",
            "  \"recall_macro\": 0.0,\n",
            "  \"f1_macro\": 0.0,\n",
            "  \"confusion_matrix_labels\": [\n",
            "    \"1\",\n",
            "    \"2\",\n",
            "    \"A\",\n",
            "    \"B\",\n",
            "    \"C\",\n",
            "    \"D\"\n",
            "  ],\n",
            "  \"confusion_matrix\": [\n",
            "    [\n",
            "      0,\n",
            "      0,\n",
            "      0,\n",
            "      0,\n",
            "      0,\n",
            "      0\n",
            "    ],\n",
            "    [\n",
            "      0,\n",
            "      0,\n",
            "      0,\n",
            "      0,\n",
            "      0,\n",
            "      0\n",
            "    ],\n",
            "    [\n",
            "      0,\n",
            "      0,\n",
            "      0,\n",
            "      0,\n",
            "      0,\n",
            "      0\n",
            "    ],\n",
            "    [\n",
            "      0,\n",
            "      0,\n",
            "      0,\n",
            "      0,\n",
            "      0,\n",
            "      0\n",
            "    ],\n",
            "    [\n",
            "      0,\n",
            "      0,\n",
            "      0,\n",
            "      0,\n",
            "      0,\n",
            "      0\n",
            "    ],\n",
            "    [\n",
            "      0,\n",
            "      0,\n",
            "      0,\n",
            "      0,\n",
            "      0,\n",
            "      0\n",
            "    ]\n",
            "  ] ...\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "      id gold  pred letters  latency_s  toks_per_s raw top_logprobs  correct\n",
              "0  arc-0    D  None    ABCD   0.165362    6.047334             None    False\n",
              "1  arc-1    C  None    ABCD   0.125493    7.968554             None    False\n",
              "2  arc-2    D  None    ABCD   0.202350    4.941929             None    False\n",
              "3  arc-3    A  None    ABCD   0.110395    9.058365             None    False\n",
              "4  arc-4    B  None    ABCD   0.114341    8.745731             None    False"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f37cb9c9-f894-4221-8542-73b88e48e54a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>gold</th>\n",
              "      <th>pred</th>\n",
              "      <th>letters</th>\n",
              "      <th>latency_s</th>\n",
              "      <th>toks_per_s</th>\n",
              "      <th>raw</th>\n",
              "      <th>top_logprobs</th>\n",
              "      <th>correct</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>arc-0</td>\n",
              "      <td>D</td>\n",
              "      <td>None</td>\n",
              "      <td>ABCD</td>\n",
              "      <td>0.165362</td>\n",
              "      <td>6.047334</td>\n",
              "      <td></td>\n",
              "      <td>None</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>arc-1</td>\n",
              "      <td>C</td>\n",
              "      <td>None</td>\n",
              "      <td>ABCD</td>\n",
              "      <td>0.125493</td>\n",
              "      <td>7.968554</td>\n",
              "      <td></td>\n",
              "      <td>None</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>arc-2</td>\n",
              "      <td>D</td>\n",
              "      <td>None</td>\n",
              "      <td>ABCD</td>\n",
              "      <td>0.202350</td>\n",
              "      <td>4.941929</td>\n",
              "      <td></td>\n",
              "      <td>None</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>arc-3</td>\n",
              "      <td>A</td>\n",
              "      <td>None</td>\n",
              "      <td>ABCD</td>\n",
              "      <td>0.110395</td>\n",
              "      <td>9.058365</td>\n",
              "      <td></td>\n",
              "      <td>None</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>arc-4</td>\n",
              "      <td>B</td>\n",
              "      <td>None</td>\n",
              "      <td>ABCD</td>\n",
              "      <td>0.114341</td>\n",
              "      <td>8.745731</td>\n",
              "      <td></td>\n",
              "      <td>None</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f37cb9c9-f894-4221-8542-73b88e48e54a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f37cb9c9-f894-4221-8542-73b88e48e54a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f37cb9c9-f894-4221-8542-73b88e48e54a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-355aad78-82a9-4278-b2e4-7c420fb7a733\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-355aad78-82a9-4278-b2e4-7c420fb7a733')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-355aad78-82a9-4278-b2e4-7c420fb7a733 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "repr_error": "Out of range float values are not JSON compliant: nan"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "HellaSwag (validation, 100 ex)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "list indices must be integers or slices, not str",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-903945607.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nHellaSwag (validation, 100 ex)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mhs_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_hellaswag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"validation\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhs_metrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"...\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhs_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3713654262.py\u001b[0m in \u001b[0;36mevaluate_hellaswag\u001b[0;34m(cfg, split, limit)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mevaluate_hellaswag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRunCfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"validation\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0mrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhellaswag_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmcq_prompt_generic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"q\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"choices\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"letters\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_llm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3713654262.py\u001b[0m in \u001b[0;36mhellaswag_iter\u001b[0;34m(split, limit)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mletters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"A\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"B\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"C\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"D\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mgold_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0mgold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mletters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgold_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mgold_idx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\\nWhich ending is most plausible?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34mf\"hs-{i}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"q\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"choices\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mendings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"letters\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mletters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"gold\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgold\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Persist results (for compare across models later)"
      ],
      "metadata": {
        "id": "pwZPWRPBrbJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summary.update({\"arc_c\": arc_metrics, \"hellaswag\": hs_metrics})\n",
        "( out_dir / f\"{run_id}_arc_preds.jsonl\").write_text(arc_df.to_json(orient=\"records\", lines=True))\n",
        "( out_dir / f\"{run_id}_hs_preds.jsonl\").write_text(hs_df.to_json(orient=\"records\", lines=True))\n",
        "( out_dir / f\"{run_id}_summary.json\").write_text(json.dumps(summary, indent=2))\n",
        "print(\"Appended ARC-C + HellaSwag to:\", out_dir / f\"{run_id}_summary.json\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9uhK17nwraim",
        "outputId": "5217b690-6d8a-440b-81bf-2c61633ab394"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Appended ARC-C + HellaSwag to: llm-bench-runs/dfc8d221_summary.json\n"
          ]
        }
      ]
    }
  ]
}